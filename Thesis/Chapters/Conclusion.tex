\lhead{Conclusion}
This thesis introduced the reader to binary classification and highlighted issues that go along with it. the literature showed that oversampling with SMOTE was widely used, and produced many different variants since its introduction in 2002. Due to the fact that all these techniques relied on the same $k$-neighbours principle, it was thought this could lead to overfitting if there were few cases available. This led to Synthsonic, which uses inversible transformations and a Bayesian Network to synthesize data samples. Because these points were independent from the original data, it could offer an improvement over the SMOTE-based competitors. 

The results show that in highly imbalanced datasets, oversampling boosts the performance of the classifier in absolute terms, but are relatively small compared to the base performance. Synthsonic manages to outperform other oversamplers in categorical datasets, most likely due to the inclusion of the Bayesian Network. However, this also led to high runtimes in datasets with a high amount of features, making it less usable in these situations. These runtimes are high compared to its SMOTE-based competitors, as they average a runtime of under a second. The hypothesis that independent samples would result in better classifier performance could not be confirmed in this study. Performance varies per dataset, and there were no indications of when oversampling would be beneficial. This leads the conclusion that using XGBoost offers some protection against imbalanced data, making the effect of oversampling small. One thing to add is that oversamplers do provide a boost in recall, meaning that it makes the classifier more sensitive to positive cases, but often at a penalty in precision. This is a balance that needs to be addressed based on the situation and the application goal. 

Synthsonic does prove a better alternative compared to other generative models, and provides an interpretable model which can easily be tuned to the situation. The transformations are all inversible, and can be removed if the situation does not need them. There are still areas that still require more research to get a better understanding of oversampling. For one, this study could be repeated for datasets which are more balanced than the 27 datasets used here. As more data is available, it also provides more data for Synthsonic to improve its model which could result in higher performance. Secondly, Synthsonic performance is reliant on the Bayesian Network it creates, there is still room for improvement in the modeling of this network. Improving this part of the model would also lead to synthetic samples which resemble the original data more closely and could improve the performance further. For now, oversampling still seems a useful tool in classification, but is not yet the one-step-solution for imbalanced data.