\lhead{Conclusion}
This thesis introduced the reader to binary classification and highlighted issues that go along with it. The literature showed that oversampling with SMOTE was widely used, and produced many different variants since its introduction in 2002. Because all these techniques relied on the same $k$-neighbours principle, it was thought this could lead to overfitting if there were few cases available. This led to Synthsonic, which uses invertible transformations and a Bayesian Network to synthesize independent data samples. Because these points were independent of the original data, it could offer an improvement over the SMOTE-based competitors. 

The results show that in highly imbalanced datasets, oversampling boosts the performance of the classifier in absolute terms, but are marginal compared to the baseline performance. Synthsonic manages to outperform other oversamplers in categorical datasets, most likely due to the inclusion of the Bayesian Network. However, this also led to high runtimes in datasets with many features, making it less usable in these situations. These runtimes are high compared to its SMOTE-based competitors, as they average runtime of under a second. The hypothesis that independent samples would result in better classifier performance could not be confirmed in this study. Performance varies per dataset, and there were no indications of when oversampling would be beneficial. This concludes that using XGBoost offers some protection against imbalanced classes, making the effect of oversampling small. One thing to add is that oversamplers boost recall, meaning that it makes the classifier more sensitive to positive cases, but often at a penalty in precision. This is a balance that needs to be addressed based on the situation and the application goal, especially when a simpler classification model (i.e. K-nearest neighbours) is used. Results showed that balanced accuracy and geometric mean are improved significantly, but that this does not hold for f1 score and PR-AUC. The relation between precision and recall becomes unbalanced, where oversampling yields a high recall but low precision. This means that the classifier becomes too sensitive to the minority class and overfits to these points. The results show that using a classifier with protection against imbalanced classes, such as XGBoost, counters this effect and gives the best results.

Synthsonic does prove a better alternative than other generative models, and provides an interpretable model which can easily be tuned to the situation. The transformations are all invertible, and can be removed if the situation does not need them. However, Synthsonic does not provide similar performance when compared to SMOTE-based oversamplers. There are still areas that still require more research to get a better understanding of oversampling. For one, this study could be repeated for datasets which are more balanced than the 27 datasets used here. As more data is available, it also provides more data for Synthsonic to improve its model which could result in higher performance. Secondly, Synthsonic performance is reliant on the Bayesian Network it creates, there is still room for improvement in the modeling of this network. Improving this part of the model would also lead to synthetic samples which resemble the original data more closely and could improve the performance further. For now, oversampling still seems a useful tool in classification, but is not yet the one-step-solution for imbalanced data.