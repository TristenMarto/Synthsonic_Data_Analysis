\lhead{\emph{Introduction}}
Prediction and classification are two common tasks in data science and it is common practice to use machine learning algorithms to recognize patterns that we might miss. In practice this could mean a classifier which determines if a patient has a disease or not, or more finance related, whether a transaction is fraudulent or not. Its application is varied, but the main task at hand stays the same: to distinguish between positive or negative, fraud or no fraud, sick or not sick. These problems are called binary classification, because a distinction must be made between two classes. This topic has been studied for a long time and many methods exist for it. 

\section{Difficulties in imbalanced classification}
However, the situation is not always as simple. In many real-life situations predicting one class is more important than the other, and errors weight heavier. In the example of medical testing, not detecting a disease weighs heavier than detecting a disease which is not present. This situation is often accompanied by the fact that the training data is imbalanced. This means that the distribution between classes is not symmetric, where one class largely outnumbers the other. In most cases, the minority class is of importance and needs to be predicted, which means that classifiers have a relatively low amount of data to recognize a pattern between these points. This issue of imbalanced learning is relevant everywhere and is therefore studied a lot and knows varied approaches. Studies on classification algorithms have tried to optimize the decision making process, but this is not the only approach. Another method is to augment the data at hand, by either adding or removing data to create a better decision boundary for the classifier. Data augmentation is the focal point of this thesis, it more specifically studies methods to generate synthetic data, creating new data points which resemble their original counterparts. 

\section{Aim of the thesis}
In this thesis, the topic of imbalanced classification is central. It aims to study different methods of oversampling and its effect on classifier performance. The topic will be introduced by a general view on existing methods for imbalanced learning, followed by a more detailed view on several oversampling methods. This leads to the literature review that examines relevant articles on oversampling and comparative studies. It explains the relevance for this thesis, as well as the reason for a new oversampling model. Chapter 4 introduces Synthsonic, a probabilistic model for the synthesis of tabular data, and a competitor for existing oversamplers. This novel method will be tested on imbalanced datasets, and its performance will be measured against other methods. The experiments and results are presented in chapters 5 and 6, followed by chapters summarizing the implications of this thesis and its suggestion for future study.
