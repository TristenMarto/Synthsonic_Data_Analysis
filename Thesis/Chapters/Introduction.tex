\lhead{\emph{Introduction}}
Prediction and classification are two common tasks in data science. In some cases, these can be solved by using regression or other analytical tools. Other times, it is common practice to use machine learning algorithms to recognize patterns that we might miss. In practice, this could mean a classifier which determines if a patient has a disease or not, or a finance-related case, whether a transaction is fraudulent or not. Its application is varied, but the main task at hand stays the same: to distinguish between positive or negative, fraud or no fraud, sick or not sick. These problems are called binary classification, because a distinction must be made between two classes. This topic has been studied for a long time and many methods exist for it. 

\section{Difficulties in imbalanced classification}
However, the situation is not always as simple. In many real-life situations, predicting one class is more important than the other, and the weight of prediction errors are more significant than the other. In the example of medical testing, not detecting a disease weighs heavier than detecting a disease which is not present. This situation is often accompanied by the fact that the training data is imbalanced. This means that the distribution between classes is not symmetric, where one class largely outnumbers the other. In most cases, the minority class is important and needs to be predicted, which means that classifiers have a relatively low amount of data to recognize a pattern between these points. This issue of imbalanced learning is relevant everywhere and is therefore studied a lot and knows varied approaches. Studies on classification algorithms have tried to optimize the decision making process, but this is not the only approach. Another method is to modify the data at hand, by either adding or removing data to create a better decision boundary for the classifier. Data augmentation to aid imbalanced learning is the focal point of this thesis, it more specifically studies methods to generate synthetic data, creating new data points which resemble their original counterparts. 

\section{Research effort}
Generating synthetic data can be done in various ways. Here, a new method is proposed which generates new data samples which are independent from the original data samples, while keeping the same underlying probability distribution. The hypothesis is that these independent data samples will prevent overfitting in imbalanced datasets, and lead to an increased classifying performance. The following research question is introduced:

\begin{quote}
    To what extend does a probabilistic model creating independent new data samples aid in imbalanced classification performance?
\end{quote}

Classification performance can be interpreted in multiple ways. In the scope of this thesis, it is defined as the ability to correctly classify minority samples without sacrificing correctly classifying the majority samples.

\section{Outline of the thesis}
In this thesis, the topic of imbalanced classification is central. It aims to study different methods of oversampling and its effect on classifier performance. The topic will be introduced by a general view of existing methods for imbalanced learning, followed by a more detailed view on several oversampling methods. This leads to the literature review that examines relevant articles on data sampling techniques, using undersampling and oversampling, and comparative studies. It explains the relevance of this thesis, as well as the reason for a new oversampling model. Chapter 4 introduces Synthsonic, a probabilistic model for the synthesis of tabular data, and a competitor for existing oversamplers. This novel method will be tested on imbalanced datasets, and its performance will be measured against other methods. The experiments and results are presented in chapters 5 and 6, followed by chapters summarizing the implications of this thesis and its suggestion for future study.
