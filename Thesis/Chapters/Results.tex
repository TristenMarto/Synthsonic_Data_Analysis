\lhead{\emph{Results}}

This chapter discusses about the results from the experiment with different SMOTE variants. 

10 oversamplers, including Synthsonic, were used in classifying 27 imbalanced datasets shown earlier in table~\ref{tab:df_info}. Due to the large amount of results, it is not feasible to discuss all details of oversampling. For comparison, the average results are shown for all datasets, but also per dataset type. The datasets are separated based on the number of categorical and numerical features in that dataset.

\section{Ranking of oversamplers}
In order to create a fair ranking over all datasets, we have taken the best performance per metric per oversampler. Table~\ref{tab:ranking} overview of the performance over various datasets with different characteristics. The first assumption is that the base classifier without oversampling should perform the worst out of all on most metrics. The exception is precision, which is expected to be higher when there is a high amount of samples in the majority class. 

\begin{table}[ht]
\centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{llrlrlrlrlrlr}
    \toprule
    {rank} &        Oversampler &  balanced\_accuracy &        Oversampler &    G\_mean &        Oversampler &        f1 &        Oversampler &  precision &        Oversampler &    recall &        Oversampler &    pr\_auc \\
    \midrule
    1  &             ADASYN &           0.798285 &             ADASYN &  0.729639 &           SVMSMOTE &  0.626186 &     NoOversampling &   0.699163 &             ADASYN &  0.614653 &         Synthsonic &  0.649430 \\
    2  &              SMOTE &           0.797884 &              SMOTE &  0.729364 &             ADASYN &  0.623797 &  polynom\_fit\_SMOTE &   0.693186 &         Synthsonic &  0.614159 &           SVMSMOTE &  0.649420 \\
    3  &         synthsonic &           0.797308 &         Synthsonic &  0.726812 &              SMOTE &  0.621968 &  RandomOversampler &   0.686230 &              SMOTE &  0.614040 &             ADASYN &  0.649238 \\
    4  &           SVMSMOTE &           0.796564 &  RandomOversampler &  0.726309 &       Random\_SMOTE &  0.621696 &           SVMSMOTE &   0.681582 &  RandomOversampler &  0.613770 &  RandomOversampler &  0.648616 \\
    5  &  RandomOversampler &           0.796323 &       Random\_SMOTE &  0.725619 &    BorderlineSMOTE &  0.620457 &       Random\_SMOTE &   0.679765 &       Random\_SMOTE &  0.609360 &  polynom\_fit\_SMOTE &  0.647145 \\
    6  &       Random\_SMOTE &           0.795903 &           SVMSMOTE &  0.724931 &  RandomOversampler &  0.620094 &             ADASYN &   0.678240 &           SVMSMOTE &  0.608782 &              SMOTE &  0.647125 \\
    7  &    BorderlineSMOTE &           0.794202 &    BorderlineSMOTE &  0.724181 &         synthsonic &  0.614133 &    BorderlineSMOTE &   0.675135 &    BorderlineSMOTE &  0.605111 &       Random\_SMOTE &  0.645629 \\
    8  &  polynom\_fit\_SMOTE &           0.781970 &  polynom\_fit\_SMOTE &  0.697985 &  polynom\_fit\_SMOTE &  0.606298 &         synthsonic &   0.671432 &  polynom\_fit\_SMOTE &  0.576800 &    BorderlineSMOTE &  0.642862 \\
    9  &            SMOTENC &           0.768076 &            SMOTENC &  0.693449 &     NoOversampling &  0.580367 &              SMOTE &   0.670155 &            SMOTENC &  0.564887 &     NoOversampling &  0.641915 \\
    10 &     NoOversampling &           0.761715 &     NoOversampling &  0.653643 &            SMOTENC &  0.555265 &            SMOTENC &   0.582476 &     NoOversampling &  0.531739 &            SMOTENC &  0.571079 \\
    \bottomrule
    \end{tabular}}
\caption{A ranking of oversamplers based on their average score of all datasets}
\label{tab:ranking}
\end{table}

Next we can see from figure~\ref{fig:Total} that No oversampling scores the lowest on all metrics, in accordance with our predictions. 

\begin{figure}[htp]
\centering
    \includesvg[width=.45\textwidth]{../Plots/Total/total_balanced_accuracy.svg}\quad
    \includesvg[width=.45\textwidth]{../Plots/Total/total_G_mean.svg}
    \medskip
    \includesvg[width=.45\textwidth]{../Plots/Total/total_f1.svg} \quad
    \includesvg[width=.45\textwidth]{../Plots/Total/total_pr_auc.svg}
    \medskip
    \includesvg[width=.45\textwidth]{../Plots/Total/total_precision.svg}\quad
    \includesvg[width=.45\textwidth]{../Plots/Total/total_recall.svg}
\caption{Average metrics scores per oversampler on all datasets. All scores range from 0 (worst) to 1 (best).}
\label{fig:Total}
\end{figure}

\section{Numerical datasets}
The first subcategory of datasets are the ones with either only numerical features or a majority of them. This subset contains 17 datasets, the majority of the total 27. Regular SMOTE techniques are expected to perform better here as their techniques require continuous features to work. Figure~\ref{tab:num}. Again ADASYN scores well on balanced accuracy and Geometric mean 

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{l|rr|rr|rr|rr|rr|rr}
        \toprule
        {} &  balanced accuracy &  std &  G\_mean & std &     f1 & std &  precision &  std &  recall &  std &  pr\_auc &  std \\
        \midrule
        ADASYN            &              0.758 &                  0.037 &   0.681 &       0.059 &  0.551 &   0.071 &      0.619 &          0.076 &   0.539 &       0.072 &   0.573 &       0.062 \\
        SMOTE             &              0.758 &                  0.032 &   0.680 &       0.054 &  0.548 &   0.056 &      0.605 &          0.066 &   0.538 &       0.067 &   0.568 &       0.060 \\
        SVMSMOTE          &              0.757 &                  0.037 &   0.674 &       0.067 &  0.555 &   0.070 &      0.626 &          0.073 &   0.532 &       0.073 &   0.574 &       0.058 \\
        Random\_SMOTE      &              0.755 &                  0.030 &   0.675 &       0.051 &  0.547 &   0.050 &      0.619 &          0.093 &   0.532 &       0.060 &   0.564 &       0.057 \\
        BorderlineSMOTE   &              0.753 &                  0.036 &   0.674 &       0.059 &  0.548 &   0.065 &      0.619 &          0.075 &   0.527 &       0.071 &   0.567 &       0.064 \\
        RandomOversampler &              0.747 &                  0.037 &   0.649 &       0.057 &  0.542 &   0.063 &      0.647 &          0.087 &   0.512 &       0.074 &   0.573 &       0.052 \\
        Synthsonic        &              0.747 &                  0.037 &   0.655 &       0.063 &  0.544 &   0.059 &      0.642 &          0.087 &   0.510 &       0.076 &   0.577 &       0.054 \\
        polynom\_fit\_SMOTE &              0.738 &                  0.032 &   0.638 &       0.050 &  0.526 &   0.060 &      0.637 &          0.089 &   0.492 &       0.066 &   0.567 &       0.051 \\
        SMOTENC           &              0.732 &                  0.041 &   0.643 &       0.075 &  0.489 &   0.063 &      0.522 &          0.092 &   0.497 &       0.081 &   0.493 &       0.057 \\
        NoOversampling    &              0.711 &                  0.037 &   0.577 &       0.065 &  0.494 &   0.070 &      0.653 &          0.079 &   0.432 &       0.075 &   0.566 &       0.059 \\
    \bottomrule
    \end{tabular}}
\caption{Scores per oversampler on datasets containing exclusively or a majority of numerical features.}
\label{tab:num}
\end{table}

\section{Categorical datasets}
The second subcategory of datasets are the ones with either exclusively or a large majority of categorical datasets. This includes nine of the total 27 datasets. Results are shown in table, after selecting the highest score per dataset  The assumption was that methods such as Synthsonic or ADASYN, which don't rely on a nearest neighbour principle, to perform better on this subset.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{l|rr|rr|rr|rr|rr|rr}
        \toprule
        {} &  balanced accuracy &  std &  G\_mean & std &     f1 & std &  precision &  std &  recall &  std &  pr\_auc &  std \\
        \midrule
        Synthsonic        &              0.882 &                  0.019 &   0.850 &       0.024 &  0.734 &   0.027 &      0.722 &          0.047 &   0.790 &       0.035 &   0.772 &       0.031 \\
        RandomOversampler &              0.879 &                  0.022 &   0.857 &       0.030 &  0.752 &   0.033 &      0.752 &          0.049 &   0.786 &       0.044 &   0.776 &       0.029 \\
        ADASYN            &              0.867 &                  0.020 &   0.812 &       0.036 &  0.748 &   0.041 &      0.779 &          0.051 &   0.744 &       0.038 &   0.780 &       0.028 \\
        SMOTE             &              0.866 &                  0.020 &   0.813 &       0.036 &  0.748 &   0.039 &      0.781 &          0.055 &   0.744 &       0.040 &   0.781 &       0.031 \\
        Random\_SMOTE      &              0.866 &                  0.020 &   0.811 &       0.035 &  0.749 &   0.038 &      0.784 &          0.055 &   0.741 &       0.037 &   0.784 &       0.028 \\
        SVMSMOTE          &              0.864 &                  0.019 &   0.812 &       0.035 &  0.747 &   0.036 &      0.777 &          0.055 &   0.739 &       0.037 &   0.777 &       0.033 \\
        BorderlineSMOTE   &              0.864 &                  0.020 &   0.809 &       0.036 &  0.743 &   0.035 &      0.771 &          0.049 &   0.737 &       0.040 &   0.772 &       0.035 \\
        polynom\_fit\_SMOTE &              0.856 &                  0.020 &   0.801 &       0.035 &  0.742 &   0.033 &      0.788 &          0.053 &   0.721 &       0.041 &   0.784 &       0.032 \\
        NoOversampling    &              0.847 &                  0.025 &   0.784 &       0.043 &  0.726 &   0.040 &      0.778 &          0.055 &   0.702 &       0.050 &   0.772 &       0.035 \\
        SMOTENC           &              0.832 &                  0.030 &   0.782 &       0.037 &  0.671 &   0.050 &      0.688 &          0.073 &   0.684 &       0.057 &   0.708 &       0.055 \\
    \bottomrule
    \end{tabular}}
\caption{Scores per oversampler on datasets containing exclusively or a majority of categorical features.}
\label{tab:categorical}
\end{table}

The first observation is that SMOTE-NC performs poorly on these datasets, even scoring the lowest on F1-score, PR AUC and precision. Note that SMOTE-NC does not work on exclusively categorical features, and therefore only was used on three instead of nine datasets. Despite this, we want to portray how oversamplers perform in a wide variety of situations and try to find an overall best oversampler. For this reason the results of SMOTE-NC are included.

Comparing these results to those of the total datasets, we can see that Synthsonic scores higher on balanced accuracy and Geometric mean when compared to the total results. This means that Synthsonic is able to discern true positives and true negatives better due to oversampling. This is also reflected in the highest score in recall on categorical datasets, but when comparing to the total results we see a higher drop in precision. 

Contrary to the total results, there is a smaller trade-off between precision and recall.

\section{Mixed datasets}


\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{l|rr|rr|rr|rr|rr|rr}
        \toprule
        {} &  balanced accuracy &  std &  G\_mean & std &     f1 & std &  precision &  std &  recall &  std &  pr\_auc &  std \\
        \midrule
        SMOTE             &              0.771 &                  0.032 &   0.690 &       0.052 &  0.557 &   0.055 &      0.589 &          0.075 &   0.569 &       0.068 &   0.573 &       0.061 \\
        ADASYN            &              0.771 &                  0.036 &   0.690 &       0.061 &  0.561 &   0.072 &      0.606 &          0.091 &   0.567 &       0.070 &   0.579 &       0.065 \\
        SVMSMOTE          &              0.769 &                  0.034 &   0.689 &       0.056 &  0.560 &   0.064 &      0.593 &          0.078 &   0.562 &       0.067 &   0.568 &       0.064 \\
        SMOTENC           &              0.768 &                  0.037 &   0.693 &       0.062 &  0.555 &   0.058 &      0.582 &          0.085 &   0.565 &       0.072 &   0.571 &       0.056 \\
        Random\_SMOTE      &              0.766 &                  0.032 &   0.682 &       0.048 &  0.555 &   0.052 &      0.615 &          0.107 &   0.558 &       0.063 &   0.570 &       0.056 \\
        BorderlineSMOTE   &              0.766 &                  0.036 &   0.683 &       0.058 &  0.551 &   0.060 &      0.577 &          0.070 &   0.557 &       0.074 &   0.559 &       0.066 \\
        RandomOversampler &              0.764 &                  0.038 &   0.686 &       0.049 &  0.550 &   0.058 &      0.583 &          0.077 &   0.560 &       0.077 &   0.567 &       0.052 \\
        Synthsonic        &              0.760 &                  0.042 &   0.670 &       0.063 &  0.543 &   0.058 &      0.585 &          0.079 &   0.542 &       0.086 &   0.574 &       0.057 \\
        polynom\_fit\_SMOTE &              0.740 &                  0.035 &   0.636 &       0.048 &  0.532 &   0.057 &      0.609 &          0.084 &   0.498 &       0.071 &   0.572 &       0.061 \\
        NoOversampling    &              0.726 &                  0.042 &   0.611 &       0.063 &  0.510 &   0.069 &      0.604 &          0.086 &   0.467 &       0.086 &   0.562 &       0.069 \\
    \bottomrule
    \end{tabular}}
\caption{Scores per oversampler on datasets containing exclusively or a majority of categorical features.}
\label{tab:mixed}
\end{table}


\section{Runtime}
This section discusses runtimes

The average runtime over all 27 datasets is shown in table~\ref{tab:runtimes}. These results are achieved by oversampling each dataset using a sampling strategy of 1, which means both classes contain an equal amount of samples. From the average runtimes it can be seen that the SMOTE-based oversamplers all have a comparable average runtime. The SMOTE-based oversamplers all average under 1 second for oversampling, with the exception of SMOTENC and SVMSMOTE. These oversamplers reached an average runtime of 1.395 seconds and 2.725 seconds respectively. From table~\ref{tab:runtimes} it can be seen that Synthsonic differs significantly from its counterparts. The average runtime was 86 seconds

\begin{table}[]
    \centering
    \begin{tabular}{lrr}
    \toprule
    stdev oversampler & runtime (s) &  stdev \\
    \midrule
    RandomOversampler &    0.014 &        0.001 \\
    polynom\_fit\_SMOTE &    0.041 &        0.002 \\
    SMOTE             &    0.059 &        0.002 \\
    BorderlineSMOTE   &    0.297 &        0.005 \\
    ADASYN            &    0.397 &        0.006 \\
    Random\_SMOTE      &    0.419 &        0.015 \\
    SMOTENC           &    1.395 &        0.024 \\
    SVMSMOTE          &    2.735 &        0.057 \\
    synthsonic        &   86.641 &        0.646 \\
    \bottomrule
    \end{tabular}
    \caption{Runtime for 1:1 oversampling, averaged over all 27 datasets.}
    \label{tab:runtimes}
\end{table}



-- overall view with boxplot and table


-- relation with size 

-- relation with features

-- 

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.7\textwidth}
    \centering
    \includesvg[width=\textwidth]{../Plots/Runtime/boxplot.svg}
    \caption{A boxplot of the runtimes per oversampler over all datasets, oversampled to a ratio of 1:1.}
    \label{fig:boxplot}
\end{subfigure}
\vfill
\begin{subfigure}[b]{0.7\textwidth}
    \centering
    \includesvg[width=\textwidth]{../Plots/Runtime/runtime_vs_features.svg}
    \caption{The runtime for 1:1 oversampling, sorted from smallest to largest dataset.}
    \label{fig:runvfeat}
\end{subfigure}
\vfill
\begin{subfigure}[b]{0.7\textwidth}
    \centering
    \includesvg[width=\textwidth]{../Plots/Runtime/runtime_vs_size.svg}
    \caption{he runtime for 1:1 oversampling, sorted from fewest to most features.}
    \label{fig:runvsize}
\end{subfigure}

\end{figure}
