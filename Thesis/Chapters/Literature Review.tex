\lhead{Literature Review}
This section discusses earlier work on alternative oversampling methods compared to SMOTE. When SMOTE was introduced in 2002, it proved to be a successful way to increase sensitivity to the minority class~\cite{Chawla2002SMOTE:Technique}. During the years after its publication, many alternatives were introduced such as ADASYN~\cite{He2008ADASYN:Learning} and Borderline-SMOTE~\cite{Han2005Borderline-SMOTE:Learning}. However, all oversamplers still relied on generating linear combinations from the samples in the minority class, but changed either which points were selected or how these were combined. 

In 2019, Kovacs published an empirical study where 85 oversamplers were tested on 104 datasets~\cite{Kovacs2019AnDatasets}. To date, there does not seem to be a study with a similar scale of approach. In comparison, the original SMOTE article used 9 datasets and ADASYN used 6 datasets. In theory, selecting fewer datasets can lead to skewed results when the oversampler is more suited to the chosen datasets. The approach of Kovacs is therefore a baseline in ranking SMOTE-based oversampling techniques. One issue with the study that the 104 datasets are not all unique. Some datasets are originally a multi-class classification problem, but the study only performed binary classification. To do this, multi class datasets were split on a one-vs-all basis, combining other labels to create an artificial imbalanced dataset. This creates many small datasets, some with around 100 total samples, where there are very few samples in the minority class to use for the model.

Then Goodfellow et al. introduced a novel way of oversampling using a Generative Adversarial Network~\cite{Goodfellow2014GenerativeNets}. This deep learning method consists of a generative model $G$ which captures the data's distribution, and a discriminative model $D$ which estimates if a generated sample comes from the training data instead of $G$\footnote{Detailed description of the architecture of GANs are omitted in this thesis as it is not relevant to Synthsonic}. This approach is now used for synthesizing audio and images which are indistinguishable from their real counterparts. This led to  The success of GANs also meant that people researched its effectiveness on tabular data, where SMOTE and undersampling techniques were still the benchmark. 


In 2020, Camino et al. claimed that using Generative Models is often not worth the extra effort it takes when compared to easier techniques~\cite{CaminoOversamplingEffort}.


During the years, studies were conducted which relied on machine learning models to model the distribution of the data. This approach 

In summary, most SMOTE-based oversamplers rely on a combination of both oversampling and undersampling to achieve an optimal performance. So far there has not been a method which can deliver a significant improvement when used as an only solution. 