\lhead{Literature Review}
This section discusses earlier work on alternative oversampling methods compared to SMOTE. When SMOTE was introduced in 2002, it proved to be a successful way to increase sensitivity to the minority class~\cite{Chawla2002SMOTE:Technique}. Many alternatives were introduced after its publication, including popular methods such as ADASYN~\cite{He2008ADASYN:Learning} and Borderline-SMOTE~\cite{Han2005Borderline-SMOTE:Learning}. However, all oversamplers still relied on generating linear combinations from the samples in the minority class, but changed either which points were selected or how these were combined. 

\section{Relevant studies}
The reason that GANs work well on images and video is that the data is in a continuous-domain. These models still struggle when dealing with mixed distributions or discrete samples~\cite{Camino2020OversamplingEffort}. Camino et al. published a study showing that the improvements per performance metric are often significant when ranking the methods, but that the improvement in absolute terms is minor compared to the extra effort. One of the issues that this study address is the selection of datasets for testing. Many studies use datasets from the UCI machine learning repository~\cite{Dua2017UCIRepository}. Many of these are not suited for deep learning methods such as GANs, as they either contain less than 1.000 samples or few features. Their final point is that many of the classification problems can be solved by state-of-the-art classifiers. Due to these points, the study of Camino et al. only contained two datasets where the test F1-score was less than 0.95. These two datasets, together with another kaggle dataset, were used in a binary classification problem using the XGBoost~\cite{Chen2016XGBoost:System} classifier. 

The study compares four classification scenarios. First is the standard classification case, without any data augmentation. Secondly, the majority class is undersampled before classification. Thirdly, the data is first undersampled and then oversampled using six oversamplers from the Imbalanced-learn package~\cite{Lemaitre2017Imbalanced-learn:Learning}. Finally, the data is undersampled and then oversampled with the Generative models. The mean f1 score and standard deviation are computed over ten equally sized and disjoint folds for each scenario. Their results showed that undersampling the majority class before classification led to an increase in F1-score on training and testing data, going from 0.716 to 0.731~\cite{Camino2020OversamplingEffort}. Both the SMOTE-based oversamplers and Generative Models achieved very similar results. Therefore, the study concluded that the oversampling techniques did not significantly benefit the effort it takes to train the models. Only undersampling the majority class seemed to have the same result.

The main issue with this article is that it does not study the effect of oversampling itself. In all their experiments, the data is first undersampled before oversampling. While this is possible in the three datasets chosen for this study, it is not always possible. Many datasets that were either too small or had too few features are often found in real-life where it is not desired to remove data. 

Oversampling using generative models is still a topic with a lot of interest. In 2016, the Synthetic Data Vault was introduced as a new benchmark in data generation~\cite{Patki2016TheVault}. This article introduced a synthetic data generation ecosystem for users to model data. The synthetic Data Vault led to a new framework, SDGym\footnote{https://github.com/sdv-dev/SDGym}, for comparing tabular data generative models. The SDGym measures the quality of generated data, but it does not offer to test the performance on imbalanced learning problems.

In another study from 2019, Kovacs' published an empirical study where 85 oversamplers were tested on 104 datasets~\cite{Kovacs2019AnDatasets}. To date, there does not seem to be a study with a similar scale of approach. Most articles use fewer datasets when introducing a new oversampling method. As an example, the original SMOTE article~\cite{Chawla2002SMOTE:Technique} used 9 datasets and ADASYN~\cite{He2008ADASYN:Learning} used 6 datasets. In theory, selecting fewer datasets can lead to skewed results when the oversampler is more suited to the chosen datasets. The approach of Kovacs is therefore a baseline in ranking SMOTE-based oversampling techniques. It highlights that the goal in imbalanced learning is to improve the classification of minority samples while maintaining a good score on the majority samples~\cite{Fernandez2018LearningSets}. 

SMOTE-based oversamplers are easy-to-use and can improve classifier performance in imbalanced datasets. On the other hand, GANs are successful in image and video generation and can outscore regular oversampling techniques~\cite{Douzas2018EffectiveNetworks}. The main issue is that training can take a lot of effort and is not worth it~\cite{Camino2020OversamplingEffort}. Synthsonic is a probabilistic data generation technique which learns from the overall class distribution, like a generative model, without the time-consuming deep learning techniques. This thesis studies its effectiveness compared to SMOTE-based oversamplers on imbalanced classification problems.