\lhead{Theory}
The literature discussed in the previous chapter provides a base for Synthsonic. While SMOTE is easy to use and GANs are very successful at generating images and sound, there is still room for a probabilistic model for synthesizing tabular data due to the limitations of regular SMOTE-based oversamplers and the difficulties of GANs.

\section{Synthsonic}
This section is dedicated to the theory behind Synthsonic. Contrary to other techniques such as SMOTE or other generative algorithms, Synthsonic uses reversible mathematical transformations, making use of as much information in the dataset as possible. This has the additional benefit that Synthsonic is easier to interpret than other GANs, where the synthesis of new samples happens through black box machine learning methods. Additionally, 


Synthsonic consists out of multiple reversible transformations of features. It can handle both numerical and categorical features, and will split a dataset $X$ into a numerical and categorical set, so that $X = (X_{num}, X_{cat})$. Modelling of the dataset happens in five steps:

\begin{enumerate}
    \item Numerical features are transformed into uniformly distributed ones, using reversible steps, from which the first-order correlations have been removed.
    \item Categorical and discretized numerical features are jointly modelled with a Bayesian network.
    \item Specific features can be selected for additional modelling.
    \item A discriminative learner is trained to model the residual discrepancies observed between the input data and the Bayesian network.
    \item The discriminative learner is calibrated to reweigh samples from the Bayesian network.
\end{enumerate}

