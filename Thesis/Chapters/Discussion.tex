\lhead{Discussion}
The main goal of the thesis is to give a broad view on oversampling performance on a wide variety of highly imbalanced datasets. In doing so, it generalizes the performance which can lead to the loss of information from individual datasets. As the results have shown, performance between oversamplers is very similar with minor differences in scores. From this perspective, the thesis agrees with the article of Camino et al.~\cite{Camino2020OversamplingEffort}, stating that oversampling offers an improvement, but it is minor compared to the effort it requires. 

The scores achieved with XGBoost were often close to their oversampled versions. In cases were the scores were low, oversampling did not significantly improve it. This leads to the belief that the classifier itself is the key factor to performance, and adding additional samples does not provide the desired effect. 

This leads to the next point of oversampling with Synthsonic, as it seems that synthesizing new independent samples does not improve on other SMOTE-based oversamplers. While Synthsonic outperformed others on some datasets, it did not show a significant improvement averaged over all datasets. The assumption was that SMOTE-based oversamplers were at risk of overfitting in cases of low sample size. The independent samples generated by Synthsonic would counter this risk and therefore perform better on small datasets. However, this study did not find such a connection. 

With the results showing that oversampling offers a minor improvement, it raises the question why SMOTE oversamplers are so popular in use. Kovacs ~\cite{Kovacs2019AnDatasets} listed 85 variants of SMOTE, showing how much time and effort has been put into this topic. This study shows two reasons for this: SMOTE oversamplers are quick, with runtimes under a second, and they are easy to use. Secondly, the results can be improved more by applying undersampling as well. 

A final point of discussion is that there were no clear characteristics in the datasets showing that oversampling would be beneficial. The datasets in this study varied in size, imbalance ratio and features, but no clear connection was found between these factors and oversampling. All oversamplers performed equally well, achieving the highest scores on some datasets but being outperformed on others. The only exception that can be made here is SMOTENC, which suffers from the limitation that it can't be used on exclusively numerical or exclusively categorical datasets.

In summary, this thesis offers a comparison between SMOTE-based techniques and Synthsonic, and shows that performance is often similar on highly imbalanced datasets. There are still questions on why and when oversampling is beneficial, but these might be answered in future studies.  