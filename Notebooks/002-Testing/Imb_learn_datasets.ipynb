{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tristenmarto/Documents/Studie/Thesis/Synthsonic_data_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Evaluation import synthsonic\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import smote_variants as sv\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score, classification_report_imbalanced\n",
    "\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.metrics import (recall_score, roc_auc_score, confusion_matrix, precision_score, precision_recall_curve,\n",
    "                             f1_score, balanced_accuracy_score, accuracy_score, average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = fetch_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['ecoli', 'optical_digits', 'satimage', 'pen_digits', 'abalone', 'sick_euthyroid', 'spectrometer', 'car_eval_34', 'isolet', 'us_crime', 'yeast_ml8', 'scene', 'libras_move', 'thyroid_sick', 'coil_2000', 'arrhythmia', 'solar_flare_m0', 'oil', 'car_eval_4', 'wine_quality', 'letter_img', 'yeast_me2', 'webpage', 'ozone_level', 'mammography', 'protein_homo', 'abalone_19'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['ecoli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[0.49, 0.29, 0.48, ..., 0.56, 0.24, 0.35],\n",
       "        [0.07, 0.4 , 0.48, ..., 0.54, 0.35, 0.44],\n",
       "        [0.56, 0.4 , 0.48, ..., 0.49, 0.37, 0.46],\n",
       "        ...,\n",
       "        [0.61, 0.6 , 0.48, ..., 0.44, 0.39, 0.38],\n",
       "        [0.59, 0.61, 0.48, ..., 0.42, 0.42, 0.37],\n",
       "        [0.74, 0.74, 0.48, ..., 0.31, 0.53, 0.52]]),\n",
       " 'target': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " 'DESCR': 'ecoli'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 301, 1: 35})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "fig = plt.figure(figsize = (12,7))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_title('2 component PCA')\n",
    "\n",
    "counter = Counter(y)\n",
    "\n",
    "for label, _ in counter.items() :\n",
    "    rowix = np.where(y == label)[0]\n",
    "    ax.scatter(principalComponents[rowix, 0], principalComponents[rowix, 1], label=str(label))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.show()\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 78\n",
      "1 63\n",
      "2 2\n",
      "3 2\n",
      "4 59\n",
      "5 82\n",
      "6 77\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.shape[1]):\n",
    "    print (i, len(np.unique(X[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check sets for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecoli\n",
      "0 78\n",
      "1 63\n",
      "2 2\n",
      "3 2\n",
      "4 59\n",
      "5 82\n",
      "6 77\n",
      "--------------------------------------------\n",
      "optical_digits\n",
      "0 1\n",
      "1 9\n",
      "2 17\n",
      "3 17\n",
      "4 17\n",
      "5 17\n",
      "6 17\n",
      "7 17\n",
      "8 4\n",
      "9 17\n",
      "10 17\n",
      "11 17\n",
      "12 17\n",
      "13 17\n",
      "14 17\n",
      "15 15\n",
      "16 5\n",
      "17 17\n",
      "18 17\n",
      "19 17\n",
      "20 17\n",
      "21 17\n",
      "22 17\n",
      "23 9\n",
      "24 2\n",
      "25 17\n",
      "26 17\n",
      "27 17\n",
      "28 17\n",
      "29 17\n",
      "30 17\n",
      "31 3\n",
      "32 2\n",
      "33 16\n",
      "34 17\n",
      "35 17\n",
      "36 17\n",
      "37 17\n",
      "38 15\n",
      "39 1\n",
      "40 8\n",
      "41 17\n",
      "42 17\n",
      "43 17\n",
      "44 17\n",
      "45 17\n",
      "46 17\n",
      "47 7\n",
      "48 9\n",
      "49 17\n",
      "50 17\n",
      "51 17\n",
      "52 17\n",
      "53 17\n",
      "54 17\n",
      "55 13\n",
      "56 2\n",
      "57 11\n",
      "58 17\n",
      "59 17\n",
      "60 17\n",
      "61 17\n",
      "62 17\n",
      "63 17\n",
      "--------------------------------------------\n",
      "satimage\n",
      "0 51\n",
      "1 84\n",
      "2 76\n",
      "3 102\n",
      "4 51\n",
      "5 82\n",
      "6 76\n",
      "7 103\n",
      "8 50\n",
      "9 81\n",
      "10 78\n",
      "11 104\n",
      "12 51\n",
      "13 83\n",
      "14 78\n",
      "15 101\n",
      "16 50\n",
      "17 80\n",
      "18 77\n",
      "19 104\n",
      "20 50\n",
      "21 80\n",
      "22 78\n",
      "23 104\n",
      "24 51\n",
      "25 82\n",
      "26 75\n",
      "27 102\n",
      "28 50\n",
      "29 81\n",
      "30 77\n",
      "31 103\n",
      "32 50\n",
      "33 80\n",
      "34 77\n",
      "35 104\n",
      "--------------------------------------------\n",
      "pen_digits\n",
      "0 101\n",
      "1 96\n",
      "2 101\n",
      "3 98\n",
      "4 101\n",
      "5 101\n",
      "6 101\n",
      "7 101\n",
      "8 101\n",
      "9 101\n",
      "10 101\n",
      "11 101\n",
      "12 101\n",
      "13 101\n",
      "14 101\n",
      "15 101\n",
      "--------------------------------------------\n",
      "abalone\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 134\n",
      "4 111\n",
      "5 51\n",
      "6 2429\n",
      "7 1515\n",
      "8 880\n",
      "9 926\n",
      "--------------------------------------------\n",
      "sick_euthyroid\n",
      "0 92\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 2\n",
      "27 239\n",
      "28 2\n",
      "29 2\n",
      "30 69\n",
      "31 2\n",
      "32 2\n",
      "33 268\n",
      "34 2\n",
      "35 2\n",
      "36 158\n",
      "37 2\n",
      "38 2\n",
      "39 280\n",
      "40 2\n",
      "41 2\n",
      "--------------------------------------------\n",
      "spectrometer\n",
      "0 531\n",
      "1 531\n",
      "2 531\n",
      "3 531\n",
      "4 531\n",
      "5 531\n",
      "6 531\n",
      "7 531\n",
      "8 531\n",
      "9 531\n",
      "10 531\n",
      "11 531\n",
      "12 531\n",
      "13 531\n",
      "14 531\n",
      "15 530\n",
      "16 531\n",
      "17 531\n",
      "18 531\n",
      "19 531\n",
      "20 531\n",
      "21 531\n",
      "22 531\n",
      "23 531\n",
      "24 531\n",
      "25 531\n",
      "26 531\n",
      "27 531\n",
      "28 531\n",
      "29 531\n",
      "30 531\n",
      "31 531\n",
      "32 531\n",
      "33 531\n",
      "34 531\n",
      "35 531\n",
      "36 531\n",
      "37 531\n",
      "38 531\n",
      "39 531\n",
      "40 531\n",
      "41 531\n",
      "42 531\n",
      "43 531\n",
      "44 531\n",
      "45 531\n",
      "46 531\n",
      "47 531\n",
      "48 531\n",
      "49 531\n",
      "50 531\n",
      "51 531\n",
      "52 531\n",
      "53 531\n",
      "54 531\n",
      "55 531\n",
      "56 531\n",
      "57 531\n",
      "58 531\n",
      "59 531\n",
      "60 531\n",
      "61 531\n",
      "62 531\n",
      "63 531\n",
      "64 531\n",
      "65 531\n",
      "66 531\n",
      "67 531\n",
      "68 531\n",
      "69 531\n",
      "70 531\n",
      "71 531\n",
      "72 531\n",
      "73 531\n",
      "74 531\n",
      "75 531\n",
      "76 531\n",
      "77 531\n",
      "78 531\n",
      "79 531\n",
      "80 531\n",
      "81 531\n",
      "82 531\n",
      "83 531\n",
      "84 531\n",
      "85 531\n",
      "86 531\n",
      "87 531\n",
      "88 531\n",
      "89 531\n",
      "90 531\n",
      "91 531\n",
      "92 531\n",
      "--------------------------------------------\n",
      "car_eval_34\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "--------------------------------------------\n",
      "isolet\n",
      "0 3570\n",
      "1 4255\n",
      "2 4252\n",
      "3 4010\n",
      "4 4738\n",
      "5 5020\n",
      "6 5180\n",
      "7 5172\n",
      "8 5044\n",
      "9 4988\n",
      "10 4921\n",
      "11 4881\n",
      "12 4902\n",
      "13 5019\n",
      "14 4876\n",
      "15 4830\n",
      "16 4662\n",
      "17 4428\n",
      "18 4214\n",
      "19 3928\n",
      "20 3826\n",
      "21 3944\n",
      "22 4118\n",
      "23 4222\n",
      "24 4278\n",
      "25 4284\n",
      "26 4330\n",
      "27 4435\n",
      "28 4478\n",
      "29 4636\n",
      "30 4756\n",
      "31 4848\n",
      "32 3353\n",
      "33 3994\n",
      "34 4153\n",
      "35 4023\n",
      "36 4792\n",
      "37 4993\n",
      "38 5144\n",
      "39 5185\n",
      "40 5136\n",
      "41 5122\n",
      "42 5057\n",
      "43 5021\n",
      "44 4987\n",
      "45 5004\n",
      "46 4965\n",
      "47 4890\n",
      "48 4740\n",
      "49 4479\n",
      "50 4292\n",
      "51 4010\n",
      "52 3917\n",
      "53 3993\n",
      "54 4041\n",
      "55 4249\n",
      "56 4310\n",
      "57 4345\n",
      "58 4434\n",
      "59 4438\n",
      "60 4543\n",
      "61 4602\n",
      "62 4763\n",
      "63 4924\n",
      "64 3333\n",
      "65 4034\n",
      "66 4242\n",
      "67 3992\n",
      "68 4799\n",
      "69 5018\n",
      "70 5054\n",
      "71 5107\n",
      "72 5208\n",
      "73 5172\n",
      "74 5147\n",
      "75 5127\n",
      "76 5040\n",
      "77 5035\n",
      "78 4979\n",
      "79 4916\n",
      "80 4809\n",
      "81 4683\n",
      "82 4410\n",
      "83 4193\n",
      "84 4131\n",
      "85 4127\n",
      "86 4255\n",
      "87 4348\n",
      "88 4384\n",
      "89 4417\n",
      "90 4437\n",
      "91 4553\n",
      "92 4574\n",
      "93 4685\n",
      "94 4880\n",
      "95 4979\n",
      "96 3442\n",
      "97 4176\n",
      "98 4199\n",
      "99 4028\n",
      "100 4853\n",
      "101 5018\n",
      "102 5040\n",
      "103 5110\n",
      "104 5138\n",
      "105 5198\n",
      "106 5135\n",
      "107 5092\n",
      "108 5030\n",
      "109 5041\n",
      "110 5012\n",
      "111 5004\n",
      "112 4905\n",
      "113 4725\n",
      "114 4543\n",
      "115 4420\n",
      "116 4292\n",
      "117 4326\n",
      "118 4407\n",
      "119 4538\n",
      "120 4587\n",
      "121 4550\n",
      "122 4632\n",
      "123 4648\n",
      "124 4731\n",
      "125 4810\n",
      "126 4859\n",
      "127 4985\n",
      "128 3682\n",
      "129 4385\n",
      "130 4203\n",
      "131 4119\n",
      "132 4915\n",
      "133 5032\n",
      "134 5064\n",
      "135 5121\n",
      "136 5118\n",
      "137 5074\n",
      "138 5087\n",
      "139 4966\n",
      "140 5027\n",
      "141 4987\n",
      "142 4987\n",
      "143 5071\n",
      "144 5005\n",
      "145 4948\n",
      "146 4753\n",
      "147 4596\n",
      "148 4531\n",
      "149 4510\n",
      "150 4586\n",
      "151 4692\n",
      "152 4748\n",
      "153 4772\n",
      "154 4832\n",
      "155 4854\n",
      "156 4897\n",
      "157 4953\n",
      "158 4969\n",
      "159 5040\n",
      "160 3981\n",
      "161 4473\n",
      "162 4171\n",
      "163 4133\n",
      "164 4959\n",
      "165 4992\n",
      "166 5029\n",
      "167 5071\n",
      "168 5008\n",
      "169 4925\n",
      "170 4867\n",
      "171 4829\n",
      "172 4757\n",
      "173 4836\n",
      "174 4970\n",
      "175 5056\n",
      "176 5103\n",
      "177 5063\n",
      "178 4927\n",
      "179 4811\n",
      "180 4716\n",
      "181 4699\n",
      "182 4730\n",
      "183 4877\n",
      "184 4849\n",
      "185 4973\n",
      "186 4993\n",
      "187 5023\n",
      "188 5043\n",
      "189 5087\n",
      "190 5040\n",
      "191 5063\n",
      "192 4541\n",
      "193 4578\n",
      "194 4355\n",
      "195 4567\n",
      "196 5048\n",
      "197 5061\n",
      "198 5048\n",
      "199 4929\n",
      "200 4873\n",
      "201 4640\n",
      "202 4553\n",
      "203 4552\n",
      "204 4536\n",
      "205 4608\n",
      "206 4751\n",
      "207 4808\n",
      "208 4933\n",
      "209 4993\n",
      "210 5013\n",
      "211 4863\n",
      "212 4865\n",
      "213 4863\n",
      "214 4900\n",
      "215 4901\n",
      "216 4905\n",
      "217 4991\n",
      "218 5040\n",
      "219 5094\n",
      "220 5091\n",
      "221 5076\n",
      "222 4959\n",
      "223 5037\n",
      "224 935\n",
      "225 901\n",
      "226 850\n",
      "227 788\n",
      "228 918\n",
      "229 996\n",
      "230 1028\n",
      "231 1039\n",
      "232 1022\n",
      "233 1026\n",
      "234 983\n",
      "235 982\n",
      "236 968\n",
      "237 952\n",
      "238 930\n",
      "239 920\n",
      "240 899\n",
      "241 871\n",
      "242 845\n",
      "243 802\n",
      "244 787\n",
      "245 788\n",
      "246 798\n",
      "247 806\n",
      "248 808\n",
      "249 817\n",
      "250 847\n",
      "251 862\n",
      "252 861\n",
      "253 900\n",
      "254 917\n",
      "255 958\n",
      "256 916\n",
      "257 935\n",
      "258 884\n",
      "259 805\n",
      "260 951\n",
      "261 1056\n",
      "262 1092\n",
      "263 1105\n",
      "264 1064\n",
      "265 1103\n",
      "266 1076\n",
      "267 1059\n",
      "268 1027\n",
      "269 1007\n",
      "270 1007\n",
      "271 968\n",
      "272 969\n",
      "273 910\n",
      "274 897\n",
      "275 860\n",
      "276 849\n",
      "277 839\n",
      "278 854\n",
      "279 855\n",
      "280 886\n",
      "281 876\n",
      "282 889\n",
      "283 905\n",
      "284 910\n",
      "285 950\n",
      "286 979\n",
      "287 1026\n",
      "288 1930\n",
      "289 2242\n",
      "290 2222\n",
      "291 2077\n",
      "292 2550\n",
      "293 2727\n",
      "294 2809\n",
      "295 2837\n",
      "296 2843\n",
      "297 2858\n",
      "298 2818\n",
      "299 2782\n",
      "300 2776\n",
      "301 2726\n",
      "302 2688\n",
      "303 2726\n",
      "304 2674\n",
      "305 2580\n",
      "306 2516\n",
      "307 2440\n",
      "308 2419\n",
      "309 2392\n",
      "310 2414\n",
      "311 2434\n",
      "312 2465\n",
      "313 2474\n",
      "314 2459\n",
      "315 2510\n",
      "316 2550\n",
      "317 2597\n",
      "318 2668\n",
      "319 2734\n",
      "320 569\n",
      "321 692\n",
      "322 830\n",
      "323 970\n",
      "324 1126\n",
      "325 1272\n",
      "326 1469\n",
      "327 1655\n",
      "328 1777\n",
      "329 2097\n",
      "330 3667\n",
      "331 4219\n",
      "332 3565\n",
      "333 3497\n",
      "334 3678\n",
      "335 4011\n",
      "336 4397\n",
      "337 4697\n",
      "338 4767\n",
      "339 4687\n",
      "340 4393\n",
      "341 3488\n",
      "342 2281\n",
      "343 1690\n",
      "344 1406\n",
      "345 1205\n",
      "346 1029\n",
      "347 883\n",
      "348 785\n",
      "349 702\n",
      "350 665\n",
      "351 624\n",
      "352 607\n",
      "353 677\n",
      "354 818\n",
      "355 1010\n",
      "356 1198\n",
      "357 1435\n",
      "358 1736\n",
      "359 2152\n",
      "360 2508\n",
      "361 2842\n",
      "362 3297\n",
      "363 4102\n",
      "364 4796\n",
      "365 4153\n",
      "366 3872\n",
      "367 4036\n",
      "368 4377\n",
      "369 4811\n",
      "370 5014\n",
      "371 5009\n",
      "372 4706\n",
      "373 4046\n",
      "374 2909\n",
      "375 1839\n",
      "376 1615\n",
      "377 1501\n",
      "378 1442\n",
      "379 1402\n",
      "380 1360\n",
      "381 1327\n",
      "382 1309\n",
      "383 1237\n",
      "384 1200\n",
      "385 1154\n",
      "386 384\n",
      "387 421\n",
      "388 508\n",
      "389 603\n",
      "390 665\n",
      "391 724\n",
      "392 898\n",
      "393 1007\n",
      "394 904\n",
      "395 826\n",
      "396 841\n",
      "397 2129\n",
      "398 2220\n",
      "399 2234\n",
      "400 2268\n",
      "401 2238\n",
      "402 2236\n",
      "403 2285\n",
      "404 2259\n",
      "405 2194\n",
      "406 2045\n",
      "407 1934\n",
      "408 756\n",
      "409 891\n",
      "410 908\n",
      "411 883\n",
      "412 789\n",
      "413 699\n",
      "414 637\n",
      "415 584\n",
      "416 554\n",
      "417 532\n",
      "418 526\n",
      "419 70\n",
      "420 70\n",
      "421 71\n",
      "422 71\n",
      "423 71\n",
      "424 71\n",
      "425 71\n",
      "426 71\n",
      "427 71\n",
      "428 71\n",
      "429 71\n",
      "430 71\n",
      "431 71\n",
      "432 71\n",
      "433 71\n",
      "434 71\n",
      "435 71\n",
      "436 71\n",
      "437 71\n",
      "438 71\n",
      "439 71\n",
      "440 71\n",
      "441 71\n",
      "442 71\n",
      "443 71\n",
      "444 71\n",
      "445 71\n",
      "446 67\n",
      "447 68\n",
      "448 70\n",
      "449 65\n",
      "450 68\n",
      "451 71\n",
      "452 3967\n",
      "453 3941\n",
      "454 3892\n",
      "455 3853\n",
      "456 3827\n",
      "457 3826\n",
      "458 3772\n",
      "459 3733\n",
      "460 3713\n",
      "461 4027\n",
      "462 5009\n",
      "463 4991\n",
      "464 5037\n",
      "465 5028\n",
      "466 4989\n",
      "467 4991\n",
      "468 5002\n",
      "469 4954\n",
      "470 4946\n",
      "471 4931\n",
      "472 147\n",
      "473 166\n",
      "474 4240\n",
      "475 4652\n",
      "476 4947\n",
      "477 4744\n",
      "478 4450\n",
      "479 241\n",
      "480 3991\n",
      "481 4041\n",
      "482 4169\n",
      "483 4091\n",
      "484 4088\n",
      "485 3998\n",
      "486 3942\n",
      "487 3763\n",
      "488 3689\n",
      "489 3585\n",
      "490 3367\n",
      "491 3333\n",
      "492 3251\n",
      "493 3256\n",
      "494 3308\n",
      "495 3321\n",
      "496 3330\n",
      "497 3379\n",
      "498 3461\n",
      "499 3509\n",
      "500 3559\n",
      "501 3564\n",
      "502 3672\n",
      "503 3769\n",
      "504 3840\n",
      "505 3862\n",
      "506 3816\n",
      "507 3826\n",
      "508 3840\n",
      "509 3885\n",
      "510 3811\n",
      "511 3811\n",
      "512 4404\n",
      "513 4467\n",
      "514 4593\n",
      "515 4571\n",
      "516 4488\n",
      "517 4383\n",
      "518 4330\n",
      "519 4184\n",
      "520 4025\n",
      "521 3825\n",
      "522 3631\n",
      "523 3555\n",
      "524 3499\n",
      "525 3548\n",
      "526 3577\n",
      "527 3532\n",
      "528 3585\n",
      "529 3617\n",
      "530 3711\n",
      "531 3761\n",
      "532 3820\n",
      "533 3890\n",
      "534 4074\n",
      "535 4123\n",
      "536 4212\n",
      "537 4196\n",
      "538 4091\n",
      "539 4135\n",
      "540 4207\n",
      "541 4205\n",
      "542 4241\n",
      "543 4363\n",
      "544 4502\n",
      "545 4443\n",
      "546 4579\n",
      "547 4649\n",
      "548 4422\n",
      "549 4345\n",
      "550 4362\n",
      "551 4228\n",
      "552 3969\n",
      "553 3740\n",
      "554 3500\n",
      "555 3488\n",
      "556 3495\n",
      "557 3550\n",
      "558 3593\n",
      "559 3673\n",
      "560 3730\n",
      "561 3749\n",
      "562 3928\n",
      "563 4055\n",
      "564 4091\n",
      "565 4203\n",
      "566 4260\n",
      "567 4331\n",
      "568 4402\n",
      "569 4357\n",
      "570 4297\n",
      "571 4307\n",
      "572 4319\n",
      "573 4289\n",
      "574 4309\n",
      "575 4358\n",
      "576 4133\n",
      "577 2\n",
      "578 2\n",
      "579 2\n",
      "580 11\n",
      "581 11\n",
      "582 11\n",
      "583 57\n",
      "584 2\n",
      "585 3572\n",
      "586 3542\n",
      "587 3429\n",
      "588 3431\n",
      "589 3330\n",
      "590 3355\n",
      "591 3314\n",
      "592 3307\n",
      "593 3188\n",
      "594 3174\n",
      "595 3047\n",
      "596 3110\n",
      "597 2980\n",
      "598 3061\n",
      "599 3090\n",
      "600 3107\n",
      "601 3146\n",
      "602 3182\n",
      "603 3183\n",
      "604 3223\n",
      "605 3248\n",
      "606 3245\n",
      "607 3327\n",
      "608 3343\n",
      "609 3333\n",
      "610 3332\n",
      "611 3305\n",
      "612 3338\n",
      "613 3353\n",
      "614 3342\n",
      "615 3364\n",
      "616 3426\n",
      "--------------------------------------------\n",
      "us_crime\n",
      "0 66\n",
      "1 93\n",
      "2 100\n",
      "3 99\n",
      "4 91\n",
      "5 91\n",
      "6 93\n",
      "7 89\n",
      "8 94\n",
      "9 98\n",
      "10 67\n",
      "11 64\n",
      "12 99\n",
      "13 96\n",
      "14 99\n",
      "15 96\n",
      "16 96\n",
      "17 101\n",
      "18 93\n",
      "19 98\n",
      "20 98\n",
      "21 101\n",
      "22 91\n",
      "23 86\n",
      "24 98\n",
      "25 97\n",
      "26 94\n",
      "27 66\n",
      "28 100\n",
      "29 97\n",
      "30 99\n",
      "31 96\n",
      "32 98\n",
      "33 96\n",
      "34 100\n",
      "35 96\n",
      "36 98\n",
      "37 99\n",
      "38 98\n",
      "39 96\n",
      "40 91\n",
      "41 94\n",
      "42 92\n",
      "43 101\n",
      "44 97\n",
      "45 99\n",
      "46 96\n",
      "47 95\n",
      "48 98\n",
      "49 55\n",
      "50 97\n",
      "51 47\n",
      "52 99\n",
      "53 100\n",
      "54 97\n",
      "55 97\n",
      "56 95\n",
      "57 97\n",
      "58 98\n",
      "59 100\n",
      "60 98\n",
      "61 94\n",
      "62 99\n",
      "63 96\n",
      "64 96\n",
      "65 94\n",
      "66 98\n",
      "67 100\n",
      "68 94\n",
      "69 100\n",
      "70 3\n",
      "71 70\n",
      "72 92\n",
      "73 99\n",
      "74 97\n",
      "75 98\n",
      "76 49\n",
      "77 99\n",
      "78 91\n",
      "79 99\n",
      "80 100\n",
      "81 98\n",
      "82 101\n",
      "83 99\n",
      "84 99\n",
      "85 100\n",
      "86 95\n",
      "87 97\n",
      "88 70\n",
      "89 54\n",
      "90 53\n",
      "91 96\n",
      "92 99\n",
      "93 99\n",
      "94 100\n",
      "95 97\n",
      "96 61\n",
      "97 96\n",
      "98 98\n",
      "99 80\n",
      "--------------------------------------------\n",
      "yeast_ml8\n",
      "0 2412\n",
      "1 2406\n",
      "2 2413\n",
      "3 2407\n",
      "4 2408\n",
      "5 2407\n",
      "6 2409\n",
      "7 2405\n",
      "8 2406\n",
      "9 2415\n",
      "10 2414\n",
      "11 2406\n",
      "12 2407\n",
      "13 2406\n",
      "14 2403\n",
      "15 2409\n",
      "16 2410\n",
      "17 2409\n",
      "18 2410\n",
      "19 2409\n",
      "20 2406\n",
      "21 2409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 2410\n",
      "23 2406\n",
      "24 2403\n",
      "25 2411\n",
      "26 2411\n",
      "27 2410\n",
      "28 2413\n",
      "29 2408\n",
      "30 2406\n",
      "31 2412\n",
      "32 2407\n",
      "33 2407\n",
      "34 2408\n",
      "35 2410\n",
      "36 2412\n",
      "37 2404\n",
      "38 2411\n",
      "39 2407\n",
      "40 2404\n",
      "41 2404\n",
      "42 2407\n",
      "43 2404\n",
      "44 2409\n",
      "45 2414\n",
      "46 2407\n",
      "47 2406\n",
      "48 2407\n",
      "49 2411\n",
      "50 2409\n",
      "51 2408\n",
      "52 2408\n",
      "53 2409\n",
      "54 2407\n",
      "55 2406\n",
      "56 2413\n",
      "57 2411\n",
      "58 2414\n",
      "59 2405\n",
      "60 2410\n",
      "61 2405\n",
      "62 2407\n",
      "63 2401\n",
      "64 2410\n",
      "65 2411\n",
      "66 2407\n",
      "67 2405\n",
      "68 2409\n",
      "69 2412\n",
      "70 2409\n",
      "71 2408\n",
      "72 2409\n",
      "73 2412\n",
      "74 2410\n",
      "75 2407\n",
      "76 2410\n",
      "77 2408\n",
      "78 2409\n",
      "79 2407\n",
      "80 2404\n",
      "81 2408\n",
      "82 2405\n",
      "83 2400\n",
      "84 2401\n",
      "85 2393\n",
      "86 2401\n",
      "87 2401\n",
      "88 2403\n",
      "89 2408\n",
      "90 2406\n",
      "91 2403\n",
      "92 2403\n",
      "93 2410\n",
      "94 2403\n",
      "95 2401\n",
      "96 2404\n",
      "97 2404\n",
      "98 2400\n",
      "99 2401\n",
      "100 2405\n",
      "101 2408\n",
      "102 2405\n",
      "--------------------------------------------\n",
      "scene\n",
      "0 2349\n",
      "1 2346\n",
      "2 2354\n",
      "3 2343\n",
      "4 2346\n",
      "5 2346\n",
      "6 2345\n",
      "7 2349\n",
      "8 2369\n",
      "9 2361\n",
      "10 2363\n",
      "11 2372\n",
      "12 2370\n",
      "13 2365\n",
      "14 2370\n",
      "15 2378\n",
      "16 2382\n",
      "17 2379\n",
      "18 2381\n",
      "19 2380\n",
      "20 2371\n",
      "21 2367\n",
      "22 2375\n",
      "23 2383\n",
      "24 2385\n",
      "25 2383\n",
      "26 2381\n",
      "27 2366\n",
      "28 2360\n",
      "29 2378\n",
      "30 2380\n",
      "31 2385\n",
      "32 2380\n",
      "33 2362\n",
      "34 2351\n",
      "35 2336\n",
      "36 2356\n",
      "37 2367\n",
      "38 2374\n",
      "39 2366\n",
      "40 2350\n",
      "41 2334\n",
      "42 2280\n",
      "43 2307\n",
      "44 2327\n",
      "45 2324\n",
      "46 2319\n",
      "47 2301\n",
      "48 2277\n",
      "49 2335\n",
      "50 2327\n",
      "51 2317\n",
      "52 2319\n",
      "53 2319\n",
      "54 2326\n",
      "55 2338\n",
      "56 2349\n",
      "57 2358\n",
      "58 2361\n",
      "59 2349\n",
      "60 2353\n",
      "61 2346\n",
      "62 2352\n",
      "63 2372\n",
      "64 2368\n",
      "65 2363\n",
      "66 2373\n",
      "67 2367\n",
      "68 2366\n",
      "69 2360\n",
      "70 2358\n",
      "71 2368\n",
      "72 2374\n",
      "73 2375\n",
      "74 2376\n",
      "75 2370\n",
      "76 2364\n",
      "77 2369\n",
      "78 2361\n",
      "79 2369\n",
      "80 2364\n",
      "81 2370\n",
      "82 2359\n",
      "83 2372\n",
      "84 2356\n",
      "85 2352\n",
      "86 2354\n",
      "87 2357\n",
      "88 2360\n",
      "89 2350\n",
      "90 2357\n",
      "91 2334\n",
      "92 2339\n",
      "93 2353\n",
      "94 2349\n",
      "95 2352\n",
      "96 2343\n",
      "97 2331\n",
      "98 2198\n",
      "99 2210\n",
      "100 2225\n",
      "101 2217\n",
      "102 2213\n",
      "103 2221\n",
      "104 2219\n",
      "105 2257\n",
      "106 2276\n",
      "107 2274\n",
      "108 2264\n",
      "109 2270\n",
      "110 2273\n",
      "111 2267\n",
      "112 2294\n",
      "113 2305\n",
      "114 2305\n",
      "115 2317\n",
      "116 2300\n",
      "117 2311\n",
      "118 2288\n",
      "119 2311\n",
      "120 2317\n",
      "121 2312\n",
      "122 2324\n",
      "123 2308\n",
      "124 2315\n",
      "125 2307\n",
      "126 2322\n",
      "127 2327\n",
      "128 2329\n",
      "129 2325\n",
      "130 2328\n",
      "131 2324\n",
      "132 2329\n",
      "133 2343\n",
      "134 2342\n",
      "135 2338\n",
      "136 2317\n",
      "137 2329\n",
      "138 2329\n",
      "139 2337\n",
      "140 2316\n",
      "141 2329\n",
      "142 2331\n",
      "143 2313\n",
      "144 2324\n",
      "145 2328\n",
      "146 2304\n",
      "147 2377\n",
      "148 2372\n",
      "149 2366\n",
      "150 2367\n",
      "151 2370\n",
      "152 2369\n",
      "153 2378\n",
      "154 2380\n",
      "155 2381\n",
      "156 2382\n",
      "157 2384\n",
      "158 2386\n",
      "159 2390\n",
      "160 2383\n",
      "161 2381\n",
      "162 2389\n",
      "163 2389\n",
      "164 2388\n",
      "165 2393\n",
      "166 2382\n",
      "167 2391\n",
      "168 2386\n",
      "169 2388\n",
      "170 2391\n",
      "171 2383\n",
      "172 2386\n",
      "173 2388\n",
      "174 2386\n",
      "175 2388\n",
      "176 2389\n",
      "177 2383\n",
      "178 2381\n",
      "179 2384\n",
      "180 2388\n",
      "181 2388\n",
      "182 2382\n",
      "183 2384\n",
      "184 2386\n",
      "185 2381\n",
      "186 2384\n",
      "187 2379\n",
      "188 2380\n",
      "189 2368\n",
      "190 2370\n",
      "191 2377\n",
      "192 2373\n",
      "193 2373\n",
      "194 2375\n",
      "195 2368\n",
      "196 2103\n",
      "197 2096\n",
      "198 2098\n",
      "199 2115\n",
      "200 2092\n",
      "201 2089\n",
      "202 2101\n",
      "203 2201\n",
      "204 2191\n",
      "205 2196\n",
      "206 2201\n",
      "207 2207\n",
      "208 2210\n",
      "209 2220\n",
      "210 2245\n",
      "211 2237\n",
      "212 2267\n",
      "213 2260\n",
      "214 2255\n",
      "215 2248\n",
      "216 2251\n",
      "217 2269\n",
      "218 2277\n",
      "219 2274\n",
      "220 2275\n",
      "221 2280\n",
      "222 2288\n",
      "223 2266\n",
      "224 2277\n",
      "225 2281\n",
      "226 2278\n",
      "227 2275\n",
      "228 2273\n",
      "229 2270\n",
      "230 2280\n",
      "231 2268\n",
      "232 2256\n",
      "233 2271\n",
      "234 2247\n",
      "235 2241\n",
      "236 2260\n",
      "237 2263\n",
      "238 2232\n",
      "239 2234\n",
      "240 2240\n",
      "241 2241\n",
      "242 2236\n",
      "243 2233\n",
      "244 2236\n",
      "245 2363\n",
      "246 2358\n",
      "247 2334\n",
      "248 2338\n",
      "249 2346\n",
      "250 2355\n",
      "251 2351\n",
      "252 2359\n",
      "253 2356\n",
      "254 2359\n",
      "255 2354\n",
      "256 2351\n",
      "257 2352\n",
      "258 2365\n",
      "259 2358\n",
      "260 2363\n",
      "261 2354\n",
      "262 2357\n",
      "263 2354\n",
      "264 2354\n",
      "265 2365\n",
      "266 2355\n",
      "267 2347\n",
      "268 2353\n",
      "269 2350\n",
      "270 2340\n",
      "271 2347\n",
      "272 2357\n",
      "273 2368\n",
      "274 2361\n",
      "275 2356\n",
      "276 2363\n",
      "277 2364\n",
      "278 2371\n",
      "279 2369\n",
      "280 2371\n",
      "281 2374\n",
      "282 2370\n",
      "283 2368\n",
      "284 2379\n",
      "285 2371\n",
      "286 2371\n",
      "287 2363\n",
      "288 2370\n",
      "289 2365\n",
      "290 2364\n",
      "291 2372\n",
      "292 2369\n",
      "293 2365\n",
      "--------------------------------------------\n",
      "libras_move\n",
      "0 214\n",
      "1 194\n",
      "2 218\n",
      "3 189\n",
      "4 217\n",
      "5 195\n",
      "6 216\n",
      "7 200\n",
      "8 216\n",
      "9 193\n",
      "10 211\n",
      "11 197\n",
      "12 217\n",
      "13 187\n",
      "14 201\n",
      "15 194\n",
      "16 211\n",
      "17 191\n",
      "18 217\n",
      "19 189\n",
      "20 216\n",
      "21 190\n",
      "22 218\n",
      "23 185\n",
      "24 219\n",
      "25 191\n",
      "26 214\n",
      "27 186\n",
      "28 213\n",
      "29 187\n",
      "30 218\n",
      "31 194\n",
      "32 214\n",
      "33 184\n",
      "34 217\n",
      "35 179\n",
      "36 213\n",
      "37 181\n",
      "38 202\n",
      "39 184\n",
      "40 212\n",
      "41 176\n",
      "42 207\n",
      "43 175\n",
      "44 206\n",
      "45 179\n",
      "46 208\n",
      "47 172\n",
      "48 213\n",
      "49 179\n",
      "50 222\n",
      "51 182\n",
      "52 201\n",
      "53 177\n",
      "54 210\n",
      "55 178\n",
      "56 214\n",
      "57 182\n",
      "58 214\n",
      "59 189\n",
      "60 214\n",
      "61 176\n",
      "62 199\n",
      "63 184\n",
      "64 232\n",
      "65 185\n",
      "66 219\n",
      "67 176\n",
      "68 219\n",
      "69 182\n",
      "70 222\n",
      "71 183\n",
      "72 227\n",
      "73 197\n",
      "74 220\n",
      "75 192\n",
      "76 227\n",
      "77 189\n",
      "78 222\n",
      "79 196\n",
      "80 229\n",
      "81 196\n",
      "82 231\n",
      "83 195\n",
      "84 225\n",
      "85 193\n",
      "86 228\n",
      "87 211\n",
      "88 235\n",
      "89 208\n",
      "--------------------------------------------\n",
      "thyroid_sick\n",
      "0 92\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 2\n",
      "27 2\n",
      "28 2\n",
      "29 2\n",
      "30 2\n",
      "31 2\n",
      "32 2\n",
      "33 287\n",
      "34 2\n",
      "35 2\n",
      "36 69\n",
      "37 2\n",
      "38 2\n",
      "39 241\n",
      "40 2\n",
      "41 2\n",
      "42 146\n",
      "43 2\n",
      "44 2\n",
      "45 234\n",
      "46 1\n",
      "47 2\n",
      "48 2\n",
      "49 2\n",
      "50 2\n",
      "51 2\n",
      "--------------------------------------------\n",
      "coil_2000\n",
      "0 40\n",
      "1 9\n",
      "2 6\n",
      "3 6\n",
      "4 10\n",
      "5 10\n",
      "6 10\n",
      "7 6\n",
      "8 10\n",
      "9 10\n",
      "10 8\n",
      "11 10\n",
      "12 10\n",
      "13 10\n",
      "14 10\n",
      "15 10\n",
      "16 10\n",
      "17 10\n",
      "18 10\n",
      "19 6\n",
      "20 10\n",
      "21 10\n",
      "22 10\n",
      "23 10\n",
      "24 10\n",
      "25 10\n",
      "26 10\n",
      "27 10\n",
      "28 10\n",
      "29 10\n",
      "30 10\n",
      "31 10\n",
      "32 9\n",
      "33 10\n",
      "34 10\n",
      "35 10\n",
      "36 10\n",
      "37 10\n",
      "38 10\n",
      "39 10\n",
      "40 9\n",
      "41 10\n",
      "42 8\n",
      "43 4\n",
      "44 7\n",
      "45 5\n",
      "46 7\n",
      "47 4\n",
      "48 6\n",
      "49 5\n",
      "50 6\n",
      "51 6\n",
      "52 6\n",
      "53 6\n",
      "54 10\n",
      "55 7\n",
      "56 3\n",
      "57 5\n",
      "58 9\n",
      "59 4\n",
      "60 7\n",
      "61 2\n",
      "62 7\n",
      "63 5\n",
      "64 3\n",
      "65 3\n",
      "66 2\n",
      "67 9\n",
      "68 6\n",
      "69 5\n",
      "70 5\n",
      "71 4\n",
      "72 7\n",
      "73 6\n",
      "74 4\n",
      "75 7\n",
      "76 2\n",
      "77 2\n",
      "78 3\n",
      "79 8\n",
      "80 2\n",
      "81 3\n",
      "82 5\n",
      "83 3\n",
      "84 3\n",
      "--------------------------------------------\n",
      "arrhythmia\n",
      "0 77\n",
      "1 2\n",
      "2 53\n",
      "3 76\n",
      "4 67\n",
      "5 106\n",
      "6 132\n",
      "7 129\n",
      "8 90\n",
      "9 160\n",
      "10 170\n",
      "11 101\n",
      "12 134\n",
      "13 63\n",
      "14 11\n",
      "15 28\n",
      "16 20\n",
      "17 4\n",
      "18 1\n",
      "19 18\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 11\n",
      "27 26\n",
      "28 20\n",
      "29 7\n",
      "30 3\n",
      "31 17\n",
      "32 2\n",
      "33 2\n",
      "34 2\n",
      "35 2\n",
      "36 2\n",
      "37 2\n",
      "38 22\n",
      "39 25\n",
      "40 22\n",
      "41 16\n",
      "42 4\n",
      "43 22\n",
      "44 2\n",
      "45 2\n",
      "46 2\n",
      "47 2\n",
      "48 2\n",
      "49 2\n",
      "50 24\n",
      "51 17\n",
      "52 15\n",
      "53 14\n",
      "54 2\n",
      "55 21\n",
      "56 2\n",
      "57 2\n",
      "58 2\n",
      "59 2\n",
      "60 2\n",
      "61 2\n",
      "62 20\n",
      "63 29\n",
      "64 22\n",
      "65 7\n",
      "66 1\n",
      "67 22\n",
      "68 1\n",
      "69 2\n",
      "70 2\n",
      "71 2\n",
      "72 2\n",
      "73 2\n",
      "74 19\n",
      "75 25\n",
      "76 23\n",
      "77 10\n",
      "78 4\n",
      "79 21\n",
      "80 2\n",
      "81 2\n",
      "82 1\n",
      "83 2\n",
      "84 2\n",
      "85 2\n",
      "86 21\n",
      "87 17\n",
      "88 23\n",
      "89 17\n",
      "90 3\n",
      "91 25\n",
      "92 2\n",
      "93 2\n",
      "94 2\n",
      "95 2\n",
      "96 2\n",
      "97 2\n",
      "98 21\n",
      "99 18\n",
      "100 22\n",
      "101 14\n",
      "102 3\n",
      "103 23\n",
      "104 2\n",
      "105 2\n",
      "106 2\n",
      "107 2\n",
      "108 2\n",
      "109 2\n",
      "110 14\n",
      "111 19\n",
      "112 26\n",
      "113 6\n",
      "114 3\n",
      "115 16\n",
      "116 2\n",
      "117 2\n",
      "118 2\n",
      "119 2\n",
      "120 2\n",
      "121 2\n",
      "122 13\n",
      "123 19\n",
      "124 28\n",
      "125 6\n",
      "126 3\n",
      "127 14\n",
      "128 2\n",
      "129 2\n",
      "130 1\n",
      "131 1\n",
      "132 2\n",
      "133 2\n",
      "134 10\n",
      "135 23\n",
      "136 23\n",
      "137 4\n",
      "138 1\n",
      "139 18\n",
      "140 1\n",
      "141 2\n",
      "142 1\n",
      "143 2\n",
      "144 1\n",
      "145 2\n",
      "146 11\n",
      "147 25\n",
      "148 20\n",
      "149 3\n",
      "150 1\n",
      "151 17\n",
      "152 2\n",
      "153 2\n",
      "154 2\n",
      "155 1\n",
      "156 1\n",
      "157 2\n",
      "158 32\n",
      "159 17\n",
      "160 111\n",
      "161 52\n",
      "162 3\n",
      "163 1\n",
      "164 24\n",
      "165 56\n",
      "166 268\n",
      "167 296\n",
      "168 35\n",
      "169 25\n",
      "170 137\n",
      "171 58\n",
      "172 8\n",
      "173 3\n",
      "174 37\n",
      "175 68\n",
      "176 290\n",
      "177 326\n",
      "178 34\n",
      "179 62\n",
      "180 105\n",
      "181 87\n",
      "182 24\n",
      "183 6\n",
      "184 34\n",
      "185 56\n",
      "186 304\n",
      "187 318\n",
      "188 31\n",
      "189 96\n",
      "190 40\n",
      "191 53\n",
      "192 21\n",
      "193 2\n",
      "194 25\n",
      "195 60\n",
      "196 253\n",
      "197 281\n",
      "198 31\n",
      "199 37\n",
      "200 101\n",
      "201 62\n",
      "202 9\n",
      "203 1\n",
      "204 23\n",
      "205 46\n",
      "206 276\n",
      "207 289\n",
      "208 28\n",
      "209 34\n",
      "210 118\n",
      "211 64\n",
      "212 11\n",
      "213 4\n",
      "214 33\n",
      "215 54\n",
      "216 279\n",
      "217 317\n",
      "218 45\n",
      "219 62\n",
      "220 58\n",
      "221 129\n",
      "222 27\n",
      "223 4\n",
      "224 33\n",
      "225 72\n",
      "226 287\n",
      "227 309\n",
      "228 57\n",
      "229 38\n",
      "230 106\n",
      "231 171\n",
      "232 24\n",
      "233 4\n",
      "234 28\n",
      "235 107\n",
      "236 315\n",
      "237 350\n",
      "238 64\n",
      "239 26\n",
      "240 164\n",
      "241 189\n",
      "242 7\n",
      "243 3\n",
      "244 33\n",
      "245 118\n",
      "246 354\n",
      "247 377\n",
      "248 52\n",
      "249 25\n",
      "250 197\n",
      "251 147\n",
      "252 10\n",
      "253 3\n",
      "254 30\n",
      "255 103\n",
      "256 336\n",
      "257 384\n",
      "258 42\n",
      "259 25\n",
      "260 168\n",
      "261 101\n",
      "262 4\n",
      "263 1\n",
      "264 28\n",
      "265 80\n",
      "266 307\n",
      "267 348\n",
      "268 37\n",
      "269 22\n",
      "270 136\n",
      "271 57\n",
      "272 3\n",
      "273 1\n",
      "274 24\n",
      "275 71\n",
      "276 286\n",
      "277 332\n",
      "--------------------------------------------\n",
      "solar_flare_m0\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 2\n",
      "27 2\n",
      "28 2\n",
      "29 2\n",
      "30 2\n",
      "31 2\n",
      "--------------------------------------------\n",
      "oil\n",
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "--------------------------------------------\n",
      "car_eval_4\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "--------------------------------------------\n",
      "wine_quality\n",
      "0 68\n",
      "1 125\n",
      "2 87\n",
      "3 310\n",
      "4 160\n",
      "5 132\n",
      "6 251\n",
      "7 890\n",
      "8 103\n",
      "9 79\n",
      "10 103\n",
      "--------------------------------------------\n",
      "letter_img\n",
      "0 16\n",
      "1 16\n",
      "2 16\n",
      "3 16\n",
      "4 16\n",
      "5 16\n",
      "6 16\n",
      "7 16\n",
      "8 16\n",
      "9 16\n",
      "10 16\n",
      "11 16\n",
      "12 16\n",
      "13 16\n",
      "14 16\n",
      "15 16\n",
      "--------------------------------------------\n",
      "yeast_me2\n",
      "0 81\n",
      "1 79\n",
      "2 53\n",
      "3 78\n",
      "4 2\n",
      "5 3\n",
      "6 48\n",
      "7 68\n",
      "--------------------------------------------\n",
      "webpage\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 2\n",
      "27 2\n",
      "28 2\n",
      "29 2\n",
      "30 2\n",
      "31 2\n",
      "32 2\n",
      "33 2\n",
      "34 2\n",
      "35 2\n",
      "36 2\n",
      "37 2\n",
      "38 2\n",
      "39 2\n",
      "40 2\n",
      "41 2\n",
      "42 2\n",
      "43 2\n",
      "44 2\n",
      "45 2\n",
      "46 2\n",
      "47 2\n",
      "48 2\n",
      "49 2\n",
      "50 2\n",
      "51 2\n",
      "52 2\n",
      "53 2\n",
      "54 2\n",
      "55 2\n",
      "56 2\n",
      "57 2\n",
      "58 2\n",
      "59 2\n",
      "60 2\n",
      "61 2\n",
      "62 2\n",
      "63 2\n",
      "64 2\n",
      "65 2\n",
      "66 2\n",
      "67 2\n",
      "68 2\n",
      "69 2\n",
      "70 2\n",
      "71 2\n",
      "72 2\n",
      "73 2\n",
      "74 2\n",
      "75 2\n",
      "76 2\n",
      "77 2\n",
      "78 2\n",
      "79 2\n",
      "80 2\n",
      "81 2\n",
      "82 2\n",
      "83 2\n",
      "84 2\n",
      "85 2\n",
      "86 2\n",
      "87 2\n",
      "88 2\n",
      "89 2\n",
      "90 2\n",
      "91 2\n",
      "92 2\n",
      "93 2\n",
      "94 2\n",
      "95 2\n",
      "96 2\n",
      "97 2\n",
      "98 2\n",
      "99 2\n",
      "100 2\n",
      "101 2\n",
      "102 2\n",
      "103 2\n",
      "104 2\n",
      "105 2\n",
      "106 2\n",
      "107 2\n",
      "108 2\n",
      "109 2\n",
      "110 2\n",
      "111 2\n",
      "112 2\n",
      "113 2\n",
      "114 2\n",
      "115 2\n",
      "116 2\n",
      "117 2\n",
      "118 2\n",
      "119 2\n",
      "120 2\n",
      "121 2\n",
      "122 2\n",
      "123 2\n",
      "124 2\n",
      "125 2\n",
      "126 2\n",
      "127 2\n",
      "128 2\n",
      "129 2\n",
      "130 2\n",
      "131 2\n",
      "132 2\n",
      "133 2\n",
      "134 2\n",
      "135 2\n",
      "136 2\n",
      "137 2\n",
      "138 2\n",
      "139 2\n",
      "140 2\n",
      "141 2\n",
      "142 2\n",
      "143 2\n",
      "144 2\n",
      "145 2\n",
      "146 2\n",
      "147 2\n",
      "148 2\n",
      "149 2\n",
      "150 2\n",
      "151 2\n",
      "152 2\n",
      "153 2\n",
      "154 2\n",
      "155 2\n",
      "156 2\n",
      "157 2\n",
      "158 2\n",
      "159 2\n",
      "160 2\n",
      "161 2\n",
      "162 2\n",
      "163 2\n",
      "164 2\n",
      "165 2\n",
      "166 2\n",
      "167 2\n",
      "168 2\n",
      "169 2\n",
      "170 2\n",
      "171 2\n",
      "172 2\n",
      "173 2\n",
      "174 2\n",
      "175 2\n",
      "176 2\n",
      "177 2\n",
      "178 2\n",
      "179 2\n",
      "180 2\n",
      "181 2\n",
      "182 2\n",
      "183 2\n",
      "184 2\n",
      "185 2\n",
      "186 2\n",
      "187 2\n",
      "188 2\n",
      "189 2\n",
      "190 2\n",
      "191 2\n",
      "192 2\n",
      "193 2\n",
      "194 2\n",
      "195 2\n",
      "196 2\n",
      "197 2\n",
      "198 2\n",
      "199 2\n",
      "200 2\n",
      "201 2\n",
      "202 2\n",
      "203 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 2\n",
      "205 2\n",
      "206 2\n",
      "207 2\n",
      "208 2\n",
      "209 2\n",
      "210 2\n",
      "211 2\n",
      "212 2\n",
      "213 2\n",
      "214 2\n",
      "215 2\n",
      "216 2\n",
      "217 2\n",
      "218 2\n",
      "219 2\n",
      "220 2\n",
      "221 2\n",
      "222 2\n",
      "223 2\n",
      "224 2\n",
      "225 2\n",
      "226 2\n",
      "227 2\n",
      "228 2\n",
      "229 2\n",
      "230 2\n",
      "231 2\n",
      "232 2\n",
      "233 2\n",
      "234 2\n",
      "235 2\n",
      "236 2\n",
      "237 2\n",
      "238 2\n",
      "239 2\n",
      "240 2\n",
      "241 2\n",
      "242 2\n",
      "243 2\n",
      "244 2\n",
      "245 2\n",
      "246 2\n",
      "247 2\n",
      "248 2\n",
      "249 2\n",
      "250 2\n",
      "251 2\n",
      "252 2\n",
      "253 2\n",
      "254 2\n",
      "255 2\n",
      "256 2\n",
      "257 2\n",
      "258 2\n",
      "259 2\n",
      "260 2\n",
      "261 2\n",
      "262 2\n",
      "263 2\n",
      "264 2\n",
      "265 2\n",
      "266 2\n",
      "267 2\n",
      "268 2\n",
      "269 2\n",
      "270 2\n",
      "271 2\n",
      "272 2\n",
      "273 2\n",
      "274 2\n",
      "275 2\n",
      "276 2\n",
      "277 2\n",
      "278 2\n",
      "279 2\n",
      "280 2\n",
      "281 2\n",
      "282 2\n",
      "283 2\n",
      "284 2\n",
      "285 2\n",
      "286 2\n",
      "287 2\n",
      "288 2\n",
      "289 2\n",
      "290 2\n",
      "291 2\n",
      "292 2\n",
      "293 2\n",
      "294 2\n",
      "295 2\n",
      "296 2\n",
      "297 2\n",
      "298 2\n",
      "299 2\n",
      "--------------------------------------------\n",
      "ozone_level\n",
      "0 68\n",
      "1 70\n",
      "2 65\n",
      "3 66\n",
      "4 64\n",
      "5 63\n",
      "6 66\n",
      "7 67\n",
      "8 69\n",
      "9 70\n",
      "10 76\n",
      "11 77\n",
      "12 77\n",
      "13 78\n",
      "14 77\n",
      "15 78\n",
      "16 72\n",
      "17 73\n",
      "18 70\n",
      "19 65\n",
      "20 68\n",
      "21 69\n",
      "22 68\n",
      "23 65\n",
      "24 74\n",
      "25 55\n",
      "26 282\n",
      "27 284\n",
      "28 287\n",
      "29 283\n",
      "30 283\n",
      "31 292\n",
      "32 295\n",
      "33 311\n",
      "34 313\n",
      "35 314\n",
      "36 327\n",
      "37 330\n",
      "38 334\n",
      "39 335\n",
      "40 335\n",
      "41 339\n",
      "42 337\n",
      "43 329\n",
      "44 321\n",
      "45 306\n",
      "46 302\n",
      "47 294\n",
      "48 287\n",
      "49 284\n",
      "50 330\n",
      "51 296\n",
      "52 251\n",
      "53 100\n",
      "54 1289\n",
      "55 1462\n",
      "56 368\n",
      "57 245\n",
      "58 100\n",
      "59 1537\n",
      "60 1429\n",
      "61 441\n",
      "62 186\n",
      "63 100\n",
      "64 1687\n",
      "65 1510\n",
      "66 85\n",
      "67 1048\n",
      "68 657\n",
      "69 71\n",
      "70 56\n",
      "71 174\n",
      "--------------------------------------------\n",
      "mammography\n",
      "0 5435\n",
      "1 883\n",
      "2 160\n",
      "3 2800\n",
      "4 1739\n",
      "5 550\n",
      "--------------------------------------------\n",
      "protein_homo\n",
      "0 6018\n",
      "1 1948\n",
      "2 1257\n",
      "3 761\n",
      "4 1378\n",
      "5 38955\n",
      "6 843\n",
      "7 1317\n",
      "8 613\n",
      "9 605\n",
      "10 22938\n",
      "11 792\n",
      "12 784\n",
      "13 4155\n",
      "14 6118\n",
      "15 29428\n",
      "16 816\n",
      "17 778\n",
      "18 2484\n",
      "19 1011\n",
      "20 7676\n",
      "21 895\n",
      "22 1053\n",
      "23 4907\n",
      "24 6091\n",
      "25 28947\n",
      "26 805\n",
      "27 1153\n",
      "28 2819\n",
      "29 697\n",
      "30 6630\n",
      "31 1019\n",
      "32 1599\n",
      "33 6519\n",
      "34 4422\n",
      "35 29448\n",
      "36 995\n",
      "37 1707\n",
      "38 4106\n",
      "39 1109\n",
      "40 7836\n",
      "41 1027\n",
      "42 2119\n",
      "43 206\n",
      "44 467\n",
      "45 51038\n",
      "46 1546\n",
      "47 1980\n",
      "48 157\n",
      "49 254\n",
      "50 24689\n",
      "51 1172\n",
      "52 1429\n",
      "53 896\n",
      "54 1626\n",
      "55 38979\n",
      "56 877\n",
      "57 1577\n",
      "58 634\n",
      "59 609\n",
      "60 11860\n",
      "61 898\n",
      "62 1502\n",
      "63 243\n",
      "64 725\n",
      "65 43633\n",
      "66 966\n",
      "67 1492\n",
      "68 163\n",
      "69 271\n",
      "70 18557\n",
      "71 840\n",
      "72 153\n",
      "73 281\n",
      "--------------------------------------------\n",
      "abalone_19\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 134\n",
      "4 111\n",
      "5 51\n",
      "6 2429\n",
      "7 1515\n",
      "8 880\n",
      "9 926\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X in datasets.keys() :\n",
    "\n",
    "    print(X)\n",
    "\n",
    "    for i in range(datasets[X]['data'].shape[1]):\n",
    "        \n",
    "        print (i, len(np.unique(datasets[X]['data'][:, i])))\n",
    "        \n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets['optical_digits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits['data'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecoli\n",
      "-1\n",
      "[-1  1] [301  35]\n",
      "--------------------------------------------\n",
      "optical_digits\n",
      "-1\n",
      "[-1  1] [5066  554]\n",
      "--------------------------------------------\n",
      "satimage\n",
      "-1\n",
      "[-1  1] [5809  626]\n",
      "--------------------------------------------\n",
      "pen_digits\n",
      "-1\n",
      "[-1  1] [9937 1055]\n",
      "--------------------------------------------\n",
      "abalone\n",
      "-1\n",
      "[-1  1] [3786  391]\n",
      "--------------------------------------------\n",
      "sick_euthyroid\n",
      "-1\n",
      "[-1  1] [2870  293]\n",
      "--------------------------------------------\n",
      "spectrometer\n",
      "-1\n",
      "[-1  1] [486  45]\n",
      "--------------------------------------------\n",
      "car_eval_34\n",
      "-1\n",
      "[-1  1] [1594  134]\n",
      "--------------------------------------------\n",
      "isolet\n",
      "-1\n",
      "[-1  1] [7197  600]\n",
      "--------------------------------------------\n",
      "us_crime\n",
      "-1\n",
      "[-1  1] [1844  150]\n",
      "--------------------------------------------\n",
      "yeast_ml8\n",
      "-1\n",
      "[-1  1] [2239  178]\n",
      "--------------------------------------------\n",
      "scene\n",
      "-1\n",
      "[-1  1] [2230  177]\n",
      "--------------------------------------------\n",
      "libras_move\n",
      "-1\n",
      "[-1  1] [336  24]\n",
      "--------------------------------------------\n",
      "thyroid_sick\n",
      "-1\n",
      "[-1  1] [3541  231]\n",
      "--------------------------------------------\n",
      "coil_2000\n",
      "-1\n",
      "[-1  1] [9236  586]\n",
      "--------------------------------------------\n",
      "arrhythmia\n",
      "-1\n",
      "[-1  1] [427  25]\n",
      "--------------------------------------------\n",
      "solar_flare_m0\n",
      "-1\n",
      "[-1  1] [1321   68]\n",
      "--------------------------------------------\n",
      "oil\n",
      "-1\n",
      "[-1  1] [896  41]\n",
      "--------------------------------------------\n",
      "car_eval_4\n",
      "-1\n",
      "[-1  1] [1663   65]\n",
      "--------------------------------------------\n",
      "wine_quality\n",
      "-1\n",
      "[-1  1] [4715  183]\n",
      "--------------------------------------------\n",
      "letter_img\n",
      "-1\n",
      "[-1  1] [19266   734]\n",
      "--------------------------------------------\n",
      "yeast_me2\n",
      "-1\n",
      "[-1  1] [1433   51]\n",
      "--------------------------------------------\n",
      "webpage\n",
      "-1\n",
      "[-1  1] [33799   981]\n",
      "--------------------------------------------\n",
      "ozone_level\n",
      "-1\n",
      "[-1  1] [2463   73]\n",
      "--------------------------------------------\n",
      "mammography\n",
      "-1\n",
      "[-1  1] [10923   260]\n",
      "--------------------------------------------\n",
      "protein_homo\n",
      "-1\n",
      "[-1  1] [144455   1296]\n",
      "--------------------------------------------\n",
      "abalone_19\n",
      "-1\n",
      "[-1  1] [4145   32]\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X in datasets.keys() :\n",
    "\n",
    "    print(X)\n",
    "    \n",
    "    v,c = np.unique(datasets[X]['target'], return_counts=True)\n",
    "    print(v[0])\n",
    "    print(v,c)        \n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name) :\n",
    "    \n",
    "    data = datasets[name]\n",
    "\n",
    "    X,y,title = data['data'], data['target'], data['DESCR']\n",
    "    \n",
    "    return X, y, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(X,y, smote=False) :\n",
    "    \n",
    "    \n",
    "    \n",
    "    if smote == True:\n",
    "        o = sv.SMOTE(random_state = random_state)\n",
    "    \n",
    "        x_g, y_g = o.sample(X_train,y_train)\n",
    "    \n",
    "    else :\n",
    "        x_g, y_g = X_train, y_train\n",
    "        \n",
    "    \n",
    "    return x_g, y_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_test(dataset_name,xdata,ydata, X_test, y_test, clf) :\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    clf.fit(xdata,ydata)\n",
    "    \n",
    "    prediction = clf.predict(X_test)\n",
    "    proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    f_dict = {\n",
    "        'name':dataset_name,\n",
    "        'balanced acc': balanced_accuracy_score(y_test,prediction),\n",
    "        'precision': precision_score(y_test,prediction),\n",
    "        'recall': recall_score(y_test,prediction),\n",
    "        'f1_score': f1_score(y_test, prediction, average='binary'),\n",
    "        'geometric mean': geometric_mean_score(y_test, prediction),\n",
    "        'average precision' : average_precision_score(y_test, prediction)\n",
    "    }\n",
    "\n",
    "    res.append(f_dict)\n",
    "    \n",
    "    return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecoli\n",
      "[10:44:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "optical_digits\n",
      "[10:44:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satimage\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "pen_digits\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "abalone\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "sick_euthyroid\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "spectrometer\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "car_eval_34\n",
      "[10:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "isolet\n",
      "[10:44:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "us_crime\n",
      "[10:44:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "yeast_ml8\n",
      "[10:44:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene\n",
      "[10:44:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "libras_move\n",
      "[10:44:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "thyroid_sick\n",
      "[10:44:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "coil_2000\n",
      "[10:44:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "arrhythmia\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "solar_flare_m0\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "oil\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "car_eval_4\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "wine_quality\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "letter_img\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "yeast_me2\n",
      "[10:44:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "webpage\n",
      "[10:44:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "ozone_level\n",
      "[10:44:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "mammography\n",
      "[10:44:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "protein_homo\n",
      "[10:44:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "abalone_19\n",
      "[10:44:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "random_state = 5\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "totalres = []\n",
    "\n",
    "for dataset in datasets.keys() :\n",
    "    print(dataset)\n",
    "    \n",
    "    X,y,title = load_data(dataset)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = random_state)\n",
    "    \n",
    "    \n",
    "    x_g, y_g = samples(X_train,y_train, clf)\n",
    "    \n",
    "    totalres.append(metrics_test(dataset, x_g, y_g, X_test, y_test, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 10:53:50,203:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:50,203:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:50,320:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:50,320:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecoli\n",
      "[10:53:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "optical_digits\n",
      "[10:53:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:50,798:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:50,798:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satimage\n",
      "[10:53:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:51,388:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:51,388:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pen_digits\n",
      "[10:53:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:51,789:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:51,789:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone\n",
      "[10:53:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:52,048:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:52,048:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sick_euthyroid\n",
      "[10:53:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:52,399:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:52,399:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:52,512:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:52,512:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrometer\n",
      "[10:53:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "car_eval_34\n",
      "[10:53:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:53:52,754:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:53:52,754:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:53] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 10:54:03,670:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:03,670:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us_crime\n",
      "[10:54:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:04,153:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:04,153:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast_ml8\n",
      "[10:54:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:05,329:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:05,329:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene\n",
      "[10:54:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:07,754:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:07,754:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:07,847:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:07,847:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libras_move\n",
      "[10:54:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "thyroid_sick\n",
      "[10:54:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:08,286:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:08,286:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coil_2000\n",
      "[10:54:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:09,873:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:09,873:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:10,001:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:10,001:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrhythmia\n",
      "[10:54:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "solar_flare_m0\n",
      "[10:54:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:10,206:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:10,206:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:10,333:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:10,333:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil\n",
      "[10:54:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "car_eval_4\n",
      "[10:54:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:10,458:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:10,458:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_quality\n",
      "[10:54:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 10:54:10,817:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:10,817:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_img\n",
      "[10:54:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:11,681:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:11,681:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast_me2\n",
      "[10:54:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:12,021:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:12,021:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 10:54:24,294:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:24,294:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozone_level\n",
      "[10:54:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:24,766:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:24,766:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mammography\n",
      "[10:54:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "2021-06-14 10:54:25,544:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:54:25,544:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein_homo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 10:55:03,951:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n",
      "2021-06-14 10:55:03,951:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 5}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone_19\n",
      "[10:55:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "totalres_smote = []\n",
    "\n",
    "for dataset in datasets.keys() :\n",
    "    print(dataset)\n",
    "    \n",
    "    X,y,title = load_data(dataset)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = random_state)\n",
    "    \n",
    "    \n",
    "    x_g, y_g = samples(X_train,y_train, smote=True)\n",
    "    \n",
    "    totalres_smote.append(metrics_test(dataset, x_g, y_g, X_test, y_test, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>geometric mean</th>\n",
       "      <th>average precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ecoli</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical_digits</th>\n",
       "      <td>0.949403</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.948117</td>\n",
       "      <td>0.898728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satimage</th>\n",
       "      <td>0.787401</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.762379</td>\n",
       "      <td>0.515938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen_digits</th>\n",
       "      <td>0.995397</td>\n",
       "      <td>0.994253</td>\n",
       "      <td>0.991404</td>\n",
       "      <td>0.992826</td>\n",
       "      <td>0.995389</td>\n",
       "      <td>0.986533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone</th>\n",
       "      <td>0.553108</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>0.361320</td>\n",
       "      <td>0.129740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick_euthyroid</th>\n",
       "      <td>0.927657</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.893401</td>\n",
       "      <td>0.925383</td>\n",
       "      <td>0.812584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectrometer</th>\n",
       "      <td>0.872919</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.866186</td>\n",
       "      <td>0.644051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_eval_34</th>\n",
       "      <td>0.957587</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.956806</td>\n",
       "      <td>0.873122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolet</th>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.956790</td>\n",
       "      <td>0.786802</td>\n",
       "      <td>0.863510</td>\n",
       "      <td>0.885712</td>\n",
       "      <td>0.769121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us_crime</th>\n",
       "      <td>0.665466</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.589096</td>\n",
       "      <td>0.265290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast_ml8</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.528036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.242202</td>\n",
       "      <td>0.119719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libras_move</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.675070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_sick</th>\n",
       "      <td>0.936212</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.934209</td>\n",
       "      <td>0.847073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coil_2000</th>\n",
       "      <td>0.546908</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.329666</td>\n",
       "      <td>0.084365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrhythmia</th>\n",
       "      <td>0.923168</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.922531</td>\n",
       "      <td>0.514603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar_flare_m0</th>\n",
       "      <td>0.492099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>0.706035</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.649230</td>\n",
       "      <td>0.173618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_eval_4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine_quality</th>\n",
       "      <td>0.586403</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.421357</td>\n",
       "      <td>0.122433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>letter_img</th>\n",
       "      <td>0.981014</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.962343</td>\n",
       "      <td>0.976645</td>\n",
       "      <td>0.980837</td>\n",
       "      <td>0.955411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast_me2</th>\n",
       "      <td>0.596771</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.454918</td>\n",
       "      <td>0.100788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage</th>\n",
       "      <td>0.844676</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.830820</td>\n",
       "      <td>0.608920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ozone_level</th>\n",
       "      <td>0.542234</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.292587</td>\n",
       "      <td>0.102517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mammography</th>\n",
       "      <td>0.798991</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.774364</td>\n",
       "      <td>0.484960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protein_homo</th>\n",
       "      <td>0.890590</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.781473</td>\n",
       "      <td>0.861257</td>\n",
       "      <td>0.883880</td>\n",
       "      <td>0.751489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone_19</th>\n",
       "      <td>0.499271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                balanced acc  precision    recall  f1_score  geometric mean  \\\n",
       "name                                                                          \n",
       "ecoli               0.976190   0.545455  1.000000  0.705882        0.975900   \n",
       "optical_digits      0.949403   0.987805  0.900000  0.941860        0.948117   \n",
       "satimage            0.787401   0.805195  0.590476  0.681319        0.762379   \n",
       "pen_digits          0.995397   0.994253  0.991404  0.992826        0.995389   \n",
       "abalone             0.553108   0.339623  0.134328  0.192513        0.361320   \n",
       "sick_euthyroid      0.927657   0.926316  0.862745  0.893401        0.925383   \n",
       "spectrometer        0.872919   0.812500  0.764706  0.787879        0.866186   \n",
       "car_eval_34         0.957587   0.944444  0.918919  0.931507        0.956806   \n",
       "isolet              0.891929   0.956790  0.786802  0.863510        0.885712   \n",
       "us_crime            0.665466   0.583333  0.355932  0.442105        0.589096   \n",
       "yeast_ml8           0.500000   0.000000  0.000000  0.000000        0.000000   \n",
       "scene               0.528036   0.666667  0.058824  0.108108        0.242202   \n",
       "libras_move         0.833333   1.000000  0.666667  0.800000        0.816497   \n",
       "thyroid_sick        0.936212   0.958904  0.875000  0.915033        0.934209   \n",
       "coil_2000           0.546908   0.291667  0.110526  0.160305        0.329666   \n",
       "arrhythmia          0.923168   0.571429  0.888889  0.695652        0.922531   \n",
       "solar_flare_m0      0.492099   0.000000  0.000000  0.000000        0.000000   \n",
       "oil                 0.706035   0.375000  0.428571  0.400000        0.649230   \n",
       "car_eval_4          1.000000   1.000000  1.000000  1.000000        1.000000   \n",
       "wine_quality        0.586403   0.526316  0.178571  0.266667        0.421357   \n",
       "letter_img          0.981014   0.991379  0.962343  0.976645        0.980837   \n",
       "yeast_me2           0.596771   0.333333  0.210526  0.258065        0.454918   \n",
       "webpage             0.844676   0.867470  0.692308  0.770053        0.830820   \n",
       "ozone_level         0.542234   0.750000  0.085714  0.153846        0.292587   \n",
       "mammography         0.798991   0.788732  0.602151  0.682927        0.774364   \n",
       "protein_homo        0.890590   0.959184  0.781473  0.861257        0.883880   \n",
       "abalone_19          0.499271   0.000000  0.000000  0.000000        0.000000   \n",
       "\n",
       "                average precision  \n",
       "name                               \n",
       "ecoli                    0.545455  \n",
       "optical_digits           0.898728  \n",
       "satimage                 0.515938  \n",
       "pen_digits               0.986533  \n",
       "abalone                  0.129740  \n",
       "sick_euthyroid           0.812584  \n",
       "spectrometer             0.644051  \n",
       "car_eval_34              0.873122  \n",
       "isolet                   0.769121  \n",
       "us_crime                 0.265290  \n",
       "yeast_ml8                0.066416  \n",
       "scene                    0.119719  \n",
       "libras_move              0.675070  \n",
       "thyroid_sick             0.847073  \n",
       "coil_2000                0.084365  \n",
       "arrhythmia               0.514603  \n",
       "solar_flare_m0           0.034858  \n",
       "oil                      0.173618  \n",
       "car_eval_4               1.000000  \n",
       "wine_quality             0.122433  \n",
       "letter_img               0.955411  \n",
       "yeast_me2                0.100788  \n",
       "webpage                  0.608920  \n",
       "ozone_level              0.102517  \n",
       "mammography              0.484960  \n",
       "protein_homo             0.751489  \n",
       "abalone_19               0.005801  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(totalres[0])\n",
    "\n",
    "for i in range(1,len(totalres)) :\n",
    "    df = df.append(totalres[i])\n",
    "    \n",
    "df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>geometric mean</th>\n",
       "      <th>average precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ecoli</th>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.546272</td>\n",
       "      <td>0.087318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical_digits</th>\n",
       "      <td>0.891940</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.693091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satimage</th>\n",
       "      <td>0.759419</td>\n",
       "      <td>0.490040</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.533623</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.327984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen_digits</th>\n",
       "      <td>0.975460</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.957020</td>\n",
       "      <td>0.950213</td>\n",
       "      <td>0.975286</td>\n",
       "      <td>0.907086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone</th>\n",
       "      <td>0.597680</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.260062</td>\n",
       "      <td>0.525761</td>\n",
       "      <td>0.136367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick_euthyroid</th>\n",
       "      <td>0.905645</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.903485</td>\n",
       "      <td>0.640410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectrometer</th>\n",
       "      <td>0.824639</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.816043</td>\n",
       "      <td>0.431770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_eval_34</th>\n",
       "      <td>0.930560</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.928238</td>\n",
       "      <td>0.822747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolet</th>\n",
       "      <td>0.864093</td>\n",
       "      <td>0.655022</td>\n",
       "      <td>0.761421</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.857972</td>\n",
       "      <td>0.517007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us_crime</th>\n",
       "      <td>0.686314</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.394161</td>\n",
       "      <td>0.647093</td>\n",
       "      <td>0.206968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast_ml8</th>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.360659</td>\n",
       "      <td>0.067270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.553898</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.442388</td>\n",
       "      <td>0.100778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libras_move</th>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.773111</td>\n",
       "      <td>0.103641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_sick</th>\n",
       "      <td>0.923525</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.921506</td>\n",
       "      <td>0.692887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coil_2000</th>\n",
       "      <td>0.550155</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.146552</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.070324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrhythmia</th>\n",
       "      <td>0.867612</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.862949</td>\n",
       "      <td>0.432137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar_flare_m0</th>\n",
       "      <td>0.484199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>0.696134</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.642661</td>\n",
       "      <td>0.104740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_eval_4</th>\n",
       "      <td>0.921053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.917663</td>\n",
       "      <td>0.847359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine_quality</th>\n",
       "      <td>0.618474</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.192090</td>\n",
       "      <td>0.532303</td>\n",
       "      <td>0.066769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>letter_img</th>\n",
       "      <td>0.957657</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.920502</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.956936</td>\n",
       "      <td>0.803315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast_me2</th>\n",
       "      <td>0.633479</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.548059</td>\n",
       "      <td>0.091866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage</th>\n",
       "      <td>0.759559</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.248123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ozone_level</th>\n",
       "      <td>0.606128</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.495581</td>\n",
       "      <td>0.082492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mammography</th>\n",
       "      <td>0.831718</td>\n",
       "      <td>0.418301</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.295720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protein_homo</th>\n",
       "      <td>0.884116</td>\n",
       "      <td>0.387707</td>\n",
       "      <td>0.779097</td>\n",
       "      <td>0.517758</td>\n",
       "      <td>0.877857</td>\n",
       "      <td>0.303995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone_19</th>\n",
       "      <td>0.539524</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.345335</td>\n",
       "      <td>0.007029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                balanced acc  precision    recall  f1_score  geometric mean  \\\n",
       "name                                                                          \n",
       "ecoli               0.614286   0.153846  0.333333  0.210526        0.546272   \n",
       "optical_digits      0.891940   0.842105  0.800000  0.820513        0.887189   \n",
       "satimage            0.759419   0.490040  0.585714  0.533623        0.739286   \n",
       "pen_digits          0.975460   0.943503  0.957020  0.950213        0.975286   \n",
       "abalone             0.597680   0.222222  0.313433  0.260062        0.525761   \n",
       "sick_euthyroid      0.905645   0.741379  0.843137  0.788991        0.903485   \n",
       "spectrometer        0.824639   0.571429  0.705882  0.631579        0.816043   \n",
       "car_eval_34         0.930560   0.941176  0.864865  0.901408        0.928238   \n",
       "isolet              0.864093   0.655022  0.761421  0.704225        0.857972   \n",
       "us_crime            0.686314   0.346154  0.457627  0.394161        0.647093   \n",
       "yeast_ml8           0.506344   0.072072  0.150943  0.097561        0.360659   \n",
       "scene               0.553898   0.154639  0.220588  0.181818        0.442388   \n",
       "libras_move         0.781609   0.142857  0.666667  0.235294        0.773111   \n",
       "thyroid_sick        0.923525   0.793103  0.862500  0.826347        0.921506   \n",
       "coil_2000           0.550155   0.124088  0.178947  0.146552        0.406049   \n",
       "arrhythmia          0.867612   0.538462  0.777778  0.636364        0.862949   \n",
       "solar_flare_m0      0.484199   0.000000  0.000000  0.000000        0.000000   \n",
       "oil                 0.696134   0.214286  0.428571  0.285714        0.642661   \n",
       "car_eval_4          0.921053   1.000000  0.842105  0.914286        0.917663   \n",
       "wine_quality        0.618474   0.140496  0.303571  0.192090        0.532303   \n",
       "letter_img          0.957657   0.869565  0.920502  0.894309        0.956936   \n",
       "yeast_me2           0.633479   0.206897  0.315789  0.250000        0.548059   \n",
       "webpage             0.759559   0.437500  0.538462  0.482759        0.726667   \n",
       "ozone_level         0.606128   0.200000  0.257143  0.225000        0.495581   \n",
       "mammography         0.831718   0.418301  0.688172  0.520325        0.819237   \n",
       "protein_homo        0.884116   0.387707  0.779097  0.517758        0.877857   \n",
       "abalone_19          0.539524   0.015625  0.125000  0.027778        0.345335   \n",
       "\n",
       "                average precision  \n",
       "name                               \n",
       "ecoli                    0.087318  \n",
       "optical_digits           0.693091  \n",
       "satimage                 0.327984  \n",
       "pen_digits               0.907086  \n",
       "abalone                  0.136367  \n",
       "sick_euthyroid           0.640410  \n",
       "spectrometer             0.431770  \n",
       "car_eval_34              0.822747  \n",
       "isolet                   0.517007  \n",
       "us_crime                 0.206968  \n",
       "yeast_ml8                0.067270  \n",
       "scene                    0.100778  \n",
       "libras_move              0.103641  \n",
       "thyroid_sick             0.692887  \n",
       "coil_2000                0.070324  \n",
       "arrhythmia               0.432137  \n",
       "solar_flare_m0           0.034858  \n",
       "oil                      0.104740  \n",
       "car_eval_4               0.847359  \n",
       "wine_quality             0.066769  \n",
       "letter_img               0.803315  \n",
       "yeast_me2                0.091866  \n",
       "webpage                  0.248123  \n",
       "ozone_level              0.082492  \n",
       "mammography              0.295720  \n",
       "protein_homo             0.303995  \n",
       "abalone_19               0.007029  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(totalres_smote[0])\n",
    "\n",
    "for i in range(1,len(totalres_smote)) :\n",
    "    df2 = df2.append(totalres_smote[i])\n",
    "    \n",
    "df2.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_synth(X,y) :\n",
    "    \n",
    "    o = synthsonic(random_state = random_state)\n",
    "\n",
    "    x_g, y_g = o.sample(X_train,y_train)\n",
    "        \n",
    "    \n",
    "    return x_g, y_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-10 12:24:49,742:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88A28D5490}\")\n",
      "2021-06-10 12:24:49,742:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88A28D5490}\")\n",
      "n_quantiles (500) is greater than the total number of samples (495). n_quantiles is set to num samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_img\n",
      "minority class 1, min dataset (495, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building tree: 100%|| 105/105.0 [00:00<00:00, 1149.46it/s]\n",
      "Generating for node: 14: 100%|| 16/16 [00:03<00:00,  4.13it/s]\n",
      "Generating for node: 14: 100%|| 16/16 [00:01<00:00, 13.63it/s]\n",
      "2021-06-10 12:25:01,364:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n",
      "2021-06-10 12:25:01,364:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n",
      "n_quantiles (500) is greater than the total number of samples (32). n_quantiles is set to num samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast_me2\n",
      "minority class 1, min dataset (32, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building tree: 100%|| 21/21.0 [00:00<00:00, 3522.23it/s]\n",
      "Generating for node: 0:   0%|          | 0/8 [00:00<?, ?it/s]/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/pgmpy/factors/discrete/DiscreteFactor.py:522: UserWarning: Found unknown state name. Trying to switch to using all state names as state numbers\n",
      "  \"Found unknown state name. Trying to switch to using all state names as state numbers\"\n",
      "Generating for node: 4: 100%|| 8/8 [00:01<00:00,  5.26it/s]\n",
      "/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/sklearn/isotonic.py:71: UserWarning: Confidence interval of the Spearman correlation coefficient spans zero. Determination of ``increasing`` may be suspect.\n",
      "  warnings.warn(\"Confidence interval of the Spearman \"\n",
      "Generating for node: 0:   0%|          | 0/8 [00:00<?, ?it/s]/Users/tristenmarto/anaconda3/lib/python3.7/site-packages/pgmpy/factors/discrete/DiscreteFactor.py:522: UserWarning: Found unknown state name. Trying to switch to using all state names as state numbers\n",
      "  \"Found unknown state name. Trying to switch to using all state names as state numbers\"\n",
      "Generating for node: 4: 100%|| 8/8 [00:00<00:00, 19.31it/s]\n",
      "2021-06-10 12:25:05,683:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n",
      "2021-06-10 12:25:05,683:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webpage\n",
      "minority class 1, min dataset (669, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building tree: 100%|| 44551/44551.0 [00:49<00:00, 909.19it/s]\n",
      "Generating for node: 290: 100%|| 300/300 [02:42<00:00,  1.85it/s]\n",
      "Generating for node: 290: 100%|| 300/300 [00:30<00:00,  9.80it/s]\n",
      "2021-06-10 12:32:00,891:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n",
      "2021-06-10 12:32:00,891:INFO:synthsonic: Running sampling via ('synthsonic', \"{'proportion': 1.0, 'n_jobs': 1, 'distinct_threshold': -1, 'random_state': RandomState(MT19937) at 0x7F88F18D4D10}\")\n",
      "n_quantiles (500) is greater than the total number of samples (38). n_quantiles is set to num samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ozone_level\n",
      "minority class 1, min dataset (38, 72)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=72 must be between 0 and min(n_samples, n_features)=38 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-86e9d3a9abed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_synth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotalres_synth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-fb1f5eb3ee81>\u001b[0m in \u001b[0;36msamples_synth\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthsonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studie/Thesis/Synthsonic_data_analysis/Evaluation.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m                              \u001b[0mnumerical_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                              categorical_columns=[])\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# determine n_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/synthsonic-0.1.0-py3.7.egg/synthsonic/models/kde_copula_nn_pdf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Transforming numerical variables.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mX_uniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Configuring Bayesian Network (cat+num).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m    366\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    440\u001b[0m                              \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                              \u001b[0;34m\"svd_solver='full'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                              % (n_components, min(n_samples, n_features)))\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=72 must be between 0 and min(n_samples, n_features)=38 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "for dataset in a[-7:] :\n",
    "    print(dataset)\n",
    "    \n",
    "    X,y,title = load_data(dataset)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = random_state)\n",
    "    \n",
    "    \n",
    "    x_g, y_g = samples_synth(X_train,y_train)\n",
    "    \n",
    "    totalres_synth.append(metrics_test(dataset, x_g, y_g, X_test, y_test, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalres_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>geometric mean</th>\n",
       "      <th>average precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ecoli</th>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>0.223146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical_digits</th>\n",
       "      <td>0.891940</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.693091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satimage</th>\n",
       "      <td>0.759419</td>\n",
       "      <td>0.490040</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.533623</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.327984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen_digits</th>\n",
       "      <td>0.975460</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.957020</td>\n",
       "      <td>0.950213</td>\n",
       "      <td>0.975286</td>\n",
       "      <td>0.907086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abalone</th>\n",
       "      <td>0.597680</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.260062</td>\n",
       "      <td>0.525761</td>\n",
       "      <td>0.136367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick_euthyroid</th>\n",
       "      <td>0.905645</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.903485</td>\n",
       "      <td>0.640410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectrometer</th>\n",
       "      <td>0.824639</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.816043</td>\n",
       "      <td>0.431770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_eval_34</th>\n",
       "      <td>0.930560</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.928238</td>\n",
       "      <td>0.822747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolet</th>\n",
       "      <td>0.864093</td>\n",
       "      <td>0.655022</td>\n",
       "      <td>0.761421</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.857972</td>\n",
       "      <td>0.517007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us_crime</th>\n",
       "      <td>0.686314</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.394161</td>\n",
       "      <td>0.647093</td>\n",
       "      <td>0.206968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeast_ml8</th>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.360659</td>\n",
       "      <td>0.067270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.553898</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.442388</td>\n",
       "      <td>0.100778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libras_move</th>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.773111</td>\n",
       "      <td>0.103641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_sick</th>\n",
       "      <td>0.923525</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.921506</td>\n",
       "      <td>0.692887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coil_2000</th>\n",
       "      <td>0.550155</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.146552</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.070324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrhythmia</th>\n",
       "      <td>0.867612</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.862949</td>\n",
       "      <td>0.432137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar_flare_m0</th>\n",
       "      <td>0.484199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                balanced acc  precision    recall  f1_score  geometric mean  \\\n",
       "name                                                                          \n",
       "ecoli               0.790476   0.307692  0.666667  0.421053        0.780720   \n",
       "optical_digits      0.891940   0.842105  0.800000  0.820513        0.887189   \n",
       "satimage            0.759419   0.490040  0.585714  0.533623        0.739286   \n",
       "pen_digits          0.975460   0.943503  0.957020  0.950213        0.975286   \n",
       "abalone             0.597680   0.222222  0.313433  0.260062        0.525761   \n",
       "sick_euthyroid      0.905645   0.741379  0.843137  0.788991        0.903485   \n",
       "spectrometer        0.824639   0.571429  0.705882  0.631579        0.816043   \n",
       "car_eval_34         0.930560   0.941176  0.864865  0.901408        0.928238   \n",
       "isolet              0.864093   0.655022  0.761421  0.704225        0.857972   \n",
       "us_crime            0.686314   0.346154  0.457627  0.394161        0.647093   \n",
       "yeast_ml8           0.506344   0.072072  0.150943  0.097561        0.360659   \n",
       "scene               0.553898   0.154639  0.220588  0.181818        0.442388   \n",
       "libras_move         0.781609   0.142857  0.666667  0.235294        0.773111   \n",
       "thyroid_sick        0.923525   0.793103  0.862500  0.826347        0.921506   \n",
       "coil_2000           0.550155   0.124088  0.178947  0.146552        0.406049   \n",
       "arrhythmia          0.867612   0.538462  0.777778  0.636364        0.862949   \n",
       "solar_flare_m0      0.484199   0.000000  0.000000  0.000000        0.000000   \n",
       "\n",
       "                average precision  \n",
       "name                               \n",
       "ecoli                    0.223146  \n",
       "optical_digits           0.693091  \n",
       "satimage                 0.327984  \n",
       "pen_digits               0.907086  \n",
       "abalone                  0.136367  \n",
       "sick_euthyroid           0.640410  \n",
       "spectrometer             0.431770  \n",
       "car_eval_34              0.822747  \n",
       "isolet                   0.517007  \n",
       "us_crime                 0.206968  \n",
       "yeast_ml8                0.067270  \n",
       "scene                    0.100778  \n",
       "libras_move              0.103641  \n",
       "thyroid_sick             0.692887  \n",
       "coil_2000                0.070324  \n",
       "arrhythmia               0.432137  \n",
       "solar_flare_m0           0.034858  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = totalres_synth[0]\n",
    "\n",
    "for i in range(1,len(totalres_synth)) :\n",
    "    df3 = df3.append(totalres_smote[i])\n",
    "    \n",
    "df3.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecoli\n",
      "optical_digits\n",
      "satimage\n",
      "pen_digits\n",
      "abalone\n",
      "sick_euthyroid\n",
      "spectrometer\n",
      "car_eval_34\n",
      "isolet\n",
      "us_crime\n",
      "yeast_ml8\n",
      "scene\n",
      "libras_move\n",
      "thyroid_sick\n",
      "coil_2000\n",
      "arrhythmia\n",
      "solar_flare_m0\n",
      "oil\n",
      "car_eval_4\n",
      "wine_quality\n",
      "letter_img\n",
      "yeast_me2\n",
      "webpage\n",
      "ozone_level\n",
      "mammography\n",
      "protein_homo\n",
      "abalone_19\n"
     ]
    }
   ],
   "source": [
    "datasets = fetch_datasets()\n",
    "total = []\n",
    "\n",
    "for name in datasets :\n",
    "    print(name)\n",
    "    \n",
    "    x,y,name = load_data(name)\n",
    "    \n",
    "    # find categorical columns\n",
    "    cat = num = 0\n",
    "    \n",
    "    for i in range(x.shape[1]) :\n",
    "        if len(np.unique(x[:, i])) > 20 :\n",
    "            num += 1\n",
    "        else :\n",
    "            cat += 1\n",
    "    \n",
    "    \n",
    "    # imb ratio\n",
    "    v,c = np.unique(y,return_counts = True)\n",
    "    majority = c[0]\n",
    "    minority = c[1]\n",
    "    ratio = minority/x.shape[0]*100\n",
    "    \n",
    "    \n",
    "    f_dict = {\n",
    "        'dataset': name,\n",
    "        'size': x.shape[0],\n",
    "        'features': x.shape[1],\n",
    "        'numerical_features': num,\n",
    "        'categorical_features': cat,\n",
    "        'majority' : majority,\n",
    "        'minority' : minority,\n",
    "        'percentage_minority_of_dataset' : ratio\n",
    "    }\n",
    "    \n",
    "    total.append(f_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(total)\n",
    "result_path = '/Users/tristenmarto/Documents/Studie/Thesis/Synthsonic_data_analysis/CSV_results/'\n",
    "df.to_csv(result_path+\"dataset_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
